    arXiv:2309.10852v2 [cs.AI] 6 Feb 2024

    Using AI Uncertainty Quantification to Improve Human Decision-Making

     Laura R. Marusich 1                 Jonathan Z. Bakdash 2              Yan Zhou 2 Murat Kantarcioglu 2

                   Abstract
        AI Uncertainty Quantification (UQ) has the po-
         tential to improve human decision-making be-
yond AI predictions alone by providing additional
probabilistic information to users. The major-
        ity of past research on AI and human decision-
       making has concentrated on model explainability
      and interpretability, with little focus on under-
         standing the potential impact of UQ on human
decision-making.   We evaluated the impact on
         human decision-making for instance-level UQ,
calibrated using a strict scoring rule, in two online
       behavioral experiments. In the first experiment,
        our results showed that UQ was beneficial for
         decision-making performance compared to only
AI predictions.    In the second experiment, we
      found UQ had generalizable benefits for decision-
        making across a variety of representations for
      probabilistic information. These results indicate
      that implementing high quality, instance-level UQ
         for AI may improve decision-making with real
systems compared to AI predictions alone.

literature as to whether presenting AI UQ for predictions
can improve human decision-making accuracy, and how to
best communicate this uncertainty information (Lai et al.,
2021). These conflicting results may be due in part to ”a
lack of discussion on the reliability of uncertainty estimates,
sometimes referred to as calibration” (Lai et al., 2021, p.
15).
In order to resolve these questions, we use well-calibrated,
instance-level AI Uncertainty Quantification (UQ) evaluated
using a strict scoring rule (Gneiting & Raftery, 2007) using
the ground truth for class labels 1. We evaluate the impact of
this AI UQ in two pre-registered, large sample size, online
behavioral experiments assessing human decision-making.
Decision-making is measured objectively using response ac-
curacy and confidence calibration with accuracy. We found
that providing high-quality AI UQ meaningfully improves
decision-accuracy and confidence calibration over an AI
prediction alone. Additionally, the benefits of this AI UQ
appear to be generalizable – decision-making was similar
for AI UQ presented with different visualizations and types
of information. Our results indicate well-calibrated AI UQ
is beneficial for decision-making.
The paper is structured as follows. In section 2, we pro-

    1. Introduction                                                     vide the background information on uncertainty and human
                                                                        decision-making and an overview of existing techniques for
    Using AI to improve human decision-making requires effec-           AI UQ. Section 3 describes our UQ technique and exper-
       tive human-AI interaction. Recent work on human-AI inter-        imental design. In section 4, we report findings from the
     action guidelines focuses on explainability and interpretabil-     behavioral experiments comparing human decision making
        ity (Amershi et al., 2019), which may improve subjective        accuracy with or without UQ information. In section 5, we
       human ratings of trust in and usability of AI. However, a        report the impact of different visualizations of UQ informa-
       quantitative synthesis of studies found that explanations        tion. Finally, sections 6 and 7 conclude by discussing the
         may not generally improve decision accuracy beyond AI          implications of our results and future work.
       prediction alone (Schemmer et al., 2022) in many applica-        2. Background and Related Work
       tion domains. One less-explored possibility for promoting
       effective human-AI interaction is AI Uncertainty Quantifi-       2.1. Human Decision-Making and Uncertainty
      cation (UQ) for predictions. AI UQ is posited to be key for
      human decision-making (Abdar et al., 2021b; Jalaian et al.,       The possible benefit of AI UQ is supported by work in
     2019). However, there is conflicting evidence in the existing      the judgment and decision-making literature on decision-
    *Equal contribution 1DEVCOM Army Research Laboratory                making under uncertainty. This work shows that providing
     2University of Texas at Dallas, Richardson, TX. Correspondence     1We wish to highlight that for the classification task, ground
    to: Laura Cooper <laura.m.cooper20.civ@army.mil>, Yan Zhou          truths for class labels are utilized to offer well-calibrated, high
    <yan.zhou2@utdallas.edu>.                                           quality, instance-level uncertainty quantification for human subject
                                                                        experiments.

                                                                  1



                                 AI UQ to Improve Human Decision-Making

    overall prediction uncertainty enhances decision-making             et al., 2021a), including Bayesian methods and ensemble
    accuracy. For example, in weather forecasting, humans               methods (Abdar et al., 2021b).
    demonstrate higher decision-making performance when                 Monte Carlo sampling (Neal, 2012) and Markov chain
    they receive well-calibrated probabilistic information (e.g.,       Monte Carlo (Salakhutdinov & Mnih, 2008; Salimans et al.,
    a forecast with a probability of rain), compared to only de-        2015; Chen et al., 2014; Ding et al., 2014; Chen et al., 2015;
    terministic predictions (e.g., it will or will not rain) (Frick     Li et al., 2016; Gong et al., 2019) are heavily used for un-
      & Hegg, 2011; Joslyn & LeClerc, 2013; Morss et al., 2008;         certainty quantification in Bayesian techniques (Kendall &
    Nadav-Greenberg & Joslyn, 2009). However, increasing                Gal, 2017; Wang et al., 2019; Liu et al., 2019a). To estimate
    information, even when it is task-relevant, is not always           aleatoric uncertainty, a hidden variable is often proposed to
    beneficial to human decision-making performance (e.g.,              represent the underlying data point x∗ from which a given
    Marusich et al., 2016; Gigerenzer & Brighton, 2009; Al-             instance x is only one of many possible observations of x∗.
    ufaisan et al., 2021). An additional consideration is the           Parameters modeling the transformation from x∗ to x can
      way that uncertainty information is represented. In human         be sampled to obtain multiple copies of the hidden x∗. For
    decision-making, communicating uncertainty with visual              epistemic uncertainty, the distribution of model parameter θ
    representations and other intuitive methods can be especially       is often approximated during training by achieving certain
    effective (Gigerenzer et al., 2007; Hullman et al., 2018).          objective optimization, for example, the Kullback–Leibler
    Despite previous general findings that uncertainty informa-         divergence. The distribution of the prediction can be sam-
    tion is useful for decision-making, there is limited behav-         pled from the samples of the learned model parameters. The
    ioral research assessing the benefits of AI UQ, particularly        predictive uncertainty can be established from the variance
    for human decision-making accuracy. Among studies that              or entropy of the sampled predictions of the sampled hidden
    do assess objective accuracy performance (e.g., Zhang et al.,       states of a given instance.
    2020; Buc
              ¸inca et al., 2021), both the methods and results         Quantifying uncertainty on learning models from a Bayesian
    vary. In particular, the quality of the UQ calibration varies,      perspective takes many different forms. Uncertainty Poste-
      with some studies opting to simulate AI prediction confi-         rior distribution over BNN weights can be learned using vari-
    dence with wizard-of-oz techniques, and others using the            ational inference (Subedar et al., 2019; Louizos & Welling,
    prediction probabilities generated by their model, but with-        2017; Farquhar et al., 2020; Ghosh et al., 2020). On the other
    out quantifying the calibration of those probabilities. As          hand, Generative Adversarial Networks (GANs) are used
    a result, the potential benefits for AI UQ remain at least          to generate out-of-distribution (OoD) examples (Oberdiek
    somewhat of an open question (Lai et al., 2021).                    et al., 2022). Implicit neural representations (INRs) are
      There is a clear gap for behavioral studies assessing hu-         reformulated from a Bayesian perspective to allow for un-
    man decision-making performance using quantifiably well-            certainty quantification (Vasconcelos et al., 2023). Simi-
    calibrated AI UQ for predictions. Our method for AI UQ              larly, Direct Epistemic Uncertainty Prediction (DEUP) is
    uses known class labels to ensure high-quality uncertainty          proposed to address the issue that using the variance of the
    information at the instance-level, as poorly calibrated un-         Bayesian posterior does not capture the epistemic uncer-
    certainty information is likely to be detrimental to decision-      tainty induced by model misspecification (Lahlou et al.,
    making. We emphasize that the application of known class            2023). Aleatoric uncertainty and epistemic uncertainty
    labels to generate instance-level UQ aims to provide well-          have also been modeled as universal adversarial perturba-
    calibrated AI UQ for individual predictions specifically in         tions (Liu et al., 2019a).
    the context of human subject experiments. This approach is          Ensemble models can enhance the predictive accuracy, how-
    not designed for real-life deployment scenarios where class         ever, it is highly debated whether an ensemble of mod-
    labels may not be known in advance. In the next section,            els can provide a good uncertainty estimate (Abdar et al.,
     we briefly provide context of existing techniques for AI UQ,       2021b; Wilson & Izmailov, 2020; Sensoy et al., 2018). Re-
    which are often model-based and typically do not require            cently, benefits of prior functions and bootstrapping in train-
    labelled data.                                                      ing ensembles with estimate of uncertainty have been dis-

2.2. Techniques for AI UQ

Predictions by AI-based systems are subject to uncertainty
from different sources. The source of uncertainty is either
aleatoric, caused by noise in data and irreducible, or epis-
temic because of uncertain model distribution (Kendall &
Gal, 2017). Uncertainty quantification methods have been
developed to assess the reliability of AI predictions (Abdar
cussed (Dwaracherla et al., 2023). Maximizing Overall
Diversity takes into account ensemble predictions for pos-
sible future input when estimating uncertainty (Jain et al.,
2020). Random parameter initialization and data shuffling
have also been proposed to estimate the uncertainty of DNN
ensembles (Lakshminarayanan et al., 2017). A Bayesian
non-parametric ensemble (BNE) approach is proposed to
account for different sources of model uncertainty (Liu et al.,

                                                                       2



                                          AI UQ to Improve Human Decision-Making

    2019b). More details on extensive studies on quantifying            tory (Dua & Graff, 2017), described in more detail below.
    uncertainty with respect to both Bayesian and ensemble
    methods, as well as in real applications can be found in (Ab-       3.1. Datasets
    dar et al., 2021b).                                                 The Census dataset has 48,842 instances and 14 attributes.
    However, prediction probabilities are prone to overconfi-           The missing values in the dataset were replaced with the
    dence in some AI models. There is a lack of discussion              mode (the most frequent value), and the dollar amounts
    on the calibration of uncertainty estimates in the existing         were adjusted for inflation. The German Credit dataset has
    literature.                                                        1,000 instances and 20 attributes. The currency values were
    In this work, we achieved efficiency of UQ estimate by as-          converted to dollars and adjusted for inflation. The Student
    sessing the change of prediction yielded from repeatedly            Performance has 649 instances and 33 attributes. Three of
    sampling noise adjacent to a given instance, and carefully          the attributes first period grade, second period grade, and
    calibrated the uncertainty information shown to the user by         final grade were combined into one with their average. Each
    leveraging the ground truth. More precisely, we provide well-       dataset was split into training (70%) and test (30%) data
    calibrated uncertainty estimates in different visualizations        sets.
    of confidence intervals to the human participants. Unlike           We selected these datasets because they involve real-world
    the existing work discussed above, our goal is to provide           contexts that are fairly intuitive for non-expert human par-
    the uncertainty information to the human participants to un-        ticipants to reason about (e.g., will a student pass or fail a
    derstand whether well-calibrated uncertainty quantification         class?). In addition, using three datasets that vary in number
    information helps in user decision-making. To achieve this          of features and in the overall accuracy classifiers can achieve
    goal, we do not attempt to come up with a UQ method a               in their predictions ensures that our findings are not limited
    priori. Instead, we take the liberty of knowing the true labels     only to one specific dataset.
    of given instances, and simplify the problem as sampling            Several machine learning models were trained on all three
    predictive confidence from instances distorted with a small         datasets, including decision tree, logistic regression, ran-
    amount of random noise. The quality of the disclosed un-            dom forest, and support vector machine. The best set of
    certainty estimate is verified using a strictly proper scoring      hyper-parameters was determined through grid search. Ran-
    rule (Gneiting & Raftery, 2007) prior to use in two behav-          dom forest was the best in terms of overall accuracy on the
    ioral experiments. While there have been recent calls for           datasets and therefore was selected for use as the AI model
    research using UQ with human decision-making (e.g., Bhatt           in this study. The mean accuracy on the Census data is
    et al. 2021; Lai et al. 2021), the few existing studies tend        85.3%, 75.7% on the German Credit data, and 85.1% on
    to focus on qualitative or subjective assessments of human          the Student Performance data. All classification tasks were
    behavior (e.g., Prabhudesai et al. 2023). Furthermore, it           completed on an Intel® Xeon® machine with a 2.30GHz
    is not clear how useful to decision-makers the UQ infor-            CPU.
    mation provided in these studies is, due to lack of proper
    calibration.                                                        3.2. Instance-Level Predictive Uncertainty

3. Current Work
We conducted two experiments to assess the effect of provid-
ing visualizations of AI prediction UQ information upon the
accuracy and confidence of human decision-making. The
first experiment compares performance when AI uncertainty
is provided to performance when only an AI prediction, or
no AI information at all, is provided. The second experi-
ment compares decision-making performance for different
representations of AI uncertainty.

Our methods and results for the instance-level predictive
UQ and behavioral experiments are fully reproducible. See
the supplementary material for details and links.
In both experiments, we assessed our research questions
using three different publicly-available and widely-used
datasets: the Census, German Credit, and Student Perfor-
mance datasets from the UCI Machine Learning Reposi-

     Quantification
UQ methods in existing literature estimate predictive uncer-
tainty without the knowledge of the true labels of the test
instances. These methods are subject to complicated calcula-
tions, sometimes poor convergence, lack of scalability, and
sometimes, they are time and resource consuming (Abdar
et al., 2021a). In our study, we aim to provide predictive
uncertainty quantification to human decision-makers and
use the advantage of knowing the true labels in advance.
Therefore, we simplify the problem as sampling predictive
confidence from samples of x with a small random distur-
bance and verify the quality of the uncertainty estimate using
a strictly proper scoring rule (Gneiting & Raftery, 2007) be-
fore showing it to the human. Note that, without knowing
the ground truth, this treatment of UQ would be reckless
and naive. It would appear that we model a prior distribu-
tion over hypothesis as the distribution over observations
in the noisy neighborhood of a given instance. However,

                                                                       3



                                            AI UQ to Improve Human Decision-Making

    0.8      Point Prediction and Brier Score         0.8    Point Prediction and Brier Score         Point Prediction and Brier Score

    0.7                                               0.7                0.6

    0.6                                               0.6                0.5

    0.5                                               0.5
    200.4                                             0.4                0.4
                                                                         0.3
    0.3                                               0.3
                                                                         0.2
    0.2                                               0.2

    0.1                                               0.1                0.1

    0.0                                               0.0                0.0
             0  20   40  60     80               100         0  20  40   60          80          100  0  10  20  30                   40
                     Sample ID                                          Sample ID                            Sample ID
                     Census                                     German Credit                            Student Performance
    Figure 1. The Brier score of the “cloned” instances for Census (100), German Credit (100), and Student Performance (40) sampled for
    demonstration. Y -axis is the Brier score. Magenta marks the samples that are correctly predicted by the AI model, cyan marks samples
    incorrectly predicted by the model. Horizontal lines illustrate the mean of the Brier score, and its 0.5, 1, and 1.5 standard deviations.

given the true label of an instance, we can hypothesize that
observations over its n neighboring noisy samples are n
plausible fits for this instance, and confirm our hypothesis
with a strictly proper scoring rule.
Predictive uncertainty consists of data uncertainty (aleatoric)
and model uncertainty (epistemic). To model data uncer-
tainty, we sample n instances from a Gaussian distribution
within a standard deviation σ from a given instance x, as-
suming x = x∗ + η where x∗ is the clean input of x without
the random disturbance η. Thus, given a prediction function
parameterized by w, the class label of x is predicted as:
                     Z
            p(y|x, w) =    p(y|x∗, w)p(x∗|x)dx∗


The posterior p(x∗|x) is generally unknown. By assuming
η ∼ N (0, σ2, I ), we can sample from the posterior distribu-
tion        0
         given the noisy input x. In this study, we set n = 100
and σ0 = 0.1.
Similarly, for model uncertainty, given a set of training data
    (X, Y ), we assume there exists an uncertain set of m models
       with model uncertainty θ(m) ∼ p(θ|X, Y ). Hence, given an
instance x, the probability of the class label of x is:

            p(y|x, X, Y ) = Ep(θ|X,Y )[p(y|x, θ)].


In this study, we tested an ensemble of logistic regression,
support vector machine, and random forest to predict the
class label. The best uncertainty estimate, however, was
obtained by using the random forest alone, assessed by the
Brier score discussed below.
Predictive uncertainty per instance was computed for 294
randomly selected Census instances, 300 German Credit
instances, and 194 Student Performance instances, for use in
the behavioral study. Predictive uncertainty at the instance-
level was measured on random samples in the neighborhood
of the instance. More specifically, given an instance x, n
random “clones” were sampled from a Gaussian distribu-
tion within δ standard deviation from the mean x. In the
experiment, we let n = 100 and δ = 0.1 which provided
sufficient statistical significance and constrained neighbor-
hood choices. Class probabilities were computed using the
trained random forest classifier for each of the 100 samples,
and the 95% confidence interval of the class probabilities
was used as the predictive uncertainty range for instance x.
UQ computed from random forest alone was superior to that
of the ensemble of logistic regression, support vector ma-
chine, and random forest, hence was used in the behavioral
study.

Knowing the ground truth (class label) of the instances, we
can verify the quality of the simulated predictive uncertainty
using the Brier score (also referred to as Brier loss). The
Brier score measures the mean squared difference between
the predicted probability and the true outcome. For each
selected instance x, with y ∈ {0, 1} and the predicted prob-
ability pi = P r(yi = 1) for each “cloned” sample xi, we
compute the Brier score B = 1 Pi(y − pi)2 between the
predicted probability of the    n
actual label of x.              “cloned” samples and y—the

If the “cloned” samples are truly representative of x, the
computed Brier score should reflect the correctness of the


                                                                        4



                                AI UQ to Improve Human Decision-Making

    prediction made for x by the AI model M. A smaller Brier            2 O    AgeSex Job HousingSaving Checking Loan Amt Duration: Purpose
    score means more accurate predictions made for the clones                  27  male skilled own  little  moderate    1210  9  business
    of x, and therefore should correspond to a correct classifi-                                 Al Predicts: Follow Terms
    cation for x by M. We verified empirically that the Brier
    scores of the predictive uncertainty is highly correlated with                        Break Terms  60%  50%          60%  Follow Terms
    the true prediction for x by M, as shown in Figure 1. Points                                     70%                      70%
      with low Brier score corresponds to instances where M is                            80%                                    80%
    correct. In Figure 1, points in magenta are the samples                               90%                                       90%
    correctly predicted by the AI model, and points in cyan are                           100%                                            100%
    samples incorrectly predicted by the model. As can be seen,                    Choose whether you think this person will Break or Follow the terms of the loan.
     “clones” for each correctly predicted sample correspond to                                             Break Terms  Follow Terms
    low Brier score loss, and vice versa, cloned samples for
    incorrectly classified samples produce high Brier scores.
    Horizontal lines illustrate the mean of the Brier score, and        Figure 2. Example showing the information appearing in the three
    its 0.5, 1, and 1.5 standard deviations. The Brier score close      AI conditions in Experiment 1 for a trial from the German Credit
    to mean (approximately 0.25) is a highly accurate indicator         dataset condition.
    of the classification outcome. In essence, the Brier score          strategies, subjective usability, subjective task difficulty, task
    resembles the trust score (Jiang et al., 2018) that has high        understanding, and an assessment of risk literacy (Cokely
    precision at identifying correctly classified examples, and is      et al., 2012), see supplementary material).                       Most partici-
    adequate to assess the quality of the estimated UQ.                 pants completed the task in less than 20 minutes, and they

3.3. Behavioral Experiments: General Methods

We used the same experimental task across both Experiment
   1 and 2, which was developed using jsPysch (De Leeuw,
2015) and hosted on MindProbe https://mindprobe.
eu/ using Just Another Tool for Online Studies (JATOS)
https://github.com/JATOS/JATOS. Each trial of
this task included a description of an individual and a two-
alternative forced choice for the classification of that indi-
vidual. Each choice was correct on 50% of the trials, thus
chance performance for human decision-making accuracy
was 50%. In some conditions, an AI prediction or an AI pre-
diction and a visualization of prediction uncertainty would
also appear. Figure 2 shows an example of the information
appearing in the three AI conditions for a trial from the
German Credit dataset condition (see supplementary ma-
terial for more example trials). After making a decision,
participants then entered their confidence in that choice, on
a Likert scale of 1 (No Confidence) to 5 (Full Confidence).
Feedback was then displayed, indicating whether or not the
previous choice was correct.
For each dataset, we selected 50 instances with representa-
tive average AI prediction accuracies (Census: 88%, Ger-
man Credit: 76%, Student Performance: 82%). Then, for
each participant, we randomly sampled 40 of those 50 in-
stances for the block of test trials, resulting in small varia-
tions in AI accuracy for each participant.
Online participants were recruited from Prolific (https:
//www.prolific.co).         They provided informed con-
sent, viewed a series of instructional screens with examples,
completed 8 practice trials, followed by 40 test trials, and a
brief series of questionnaires (demographics, self-reported
were paid $5.00 for their participation (i.e., well above the
U.S. federal minimum hourly wage). This research received
Institutional Review Board (IRB) approval.

4. Experiment 1

Experiment 1 compared participant decision-making accu-
racy in three conditions: Control (no AI prediction infor-
mation), AI Prediction, and AI Uncertainty (AI prediction
plus a visualized point estimate of AI uncertainty), see Fig-
ure 2. All hypotheses and methods were pre-registered
(https://aspredicted.org/ZW9_Z54).
We hypothesized that for all three datasets, participant de-
cision accuracy would be highest in the AI Uncertainty
condition, followed by the AI Prediction condition, and low-
est in the Control condition. Similarly, we hypothesized
that confidence calibration (positive association between
confidence and accuracy) would be strongest in the AI Un-
certainty condition, followed by the AI Prediction condition,
and lowest in the Control condition.

4.1. Participants
We recruited nearly 50 participants in each of 9 experimen-
tal conditions, for a total of 445 participants (48.8% male,
48.5% female, 2.7% other or prefer not to answer). The
majority (68.8%) of participants were 18-44 years old.

4.2. Results and Discussion
We excluded trials with reaction times that exceeded three
standard deviations above the mean; this resulted in the
removal of 396 out of 17,800 trials across all participants.
An omnibus 3 (AI Condition) x 3 (Dataset) ANOVA for

                                                                       5



                                          AI UQ to Improve Human Decision-Making

    mean accuracy indicated a significant main effect of AI con-
    dition (F (2, 436) = 84.11, p < 0.001, η2 = 0.28; see left                             1.0    Credit                Student      Census
    side of Figure 3). This large effect size p
                                          is driven primarily
    by the differences between the Control condition and the                              200.8
    other two conditions. However, we also used Tukey’s honest
    significance test to conduct post-hoc comparisons between
    individual conditions. These comparisons showed not only                               0.6                                       Condition
    that accuracy in the AI Prediction condition was higher                                                                                                         Control
                                                                                                                                                                    Al Prediction
    than in the Control condition (t(436) = 9.91, p < 0.0001),                             0.4                                                                      Al Uncertainty
    but also that accuracy in the AI Uncertainty condition was                                    1  2  3   4  5 1  2   3          4  5 1  2  3  4                         5
                                                                                                                    Confidence Ratings
    further improved (although to a lesser extent) over the AI
    Prediction condition (t(436) = 2.36, p = 0.049).                                      Figure 4. Predicted effects (level 2/overall results of multilevel

              Experiment 1                Experiment 2                                    model) of confidence ratings, dataset, and AI condition upon ac-
     0.9                                  Needle          Dotplot                         curacy in Experiment 1. Steeper, positively-sloped lines indicate
                                                                                          better confidence calibration. Shaded areas represent 95% confi-
    200.8                       b                   I  H                                  dence intervals for the predicted values.     Experiment 2
     0.7                                                                                                Experiment 1
                                                                                                                                      Needle    Dotplot
     0.6                                                    Dataset                        8000
                                                                             Credit       2
                                                                             Student
     0.5                                                    HCensus
             Control  Al Prediction  Al Uncertainty  Point    Dist  Point    Dist          6000                                    P
                        Condition                             Condition

Figure 3. Participant accuracy in Experiments 1 (left) and 2 (right).

Error bars represent 95% confidence intervals.


There was also a significant main effect of dataset upon
accuracy (F (2, 436) = 144.72, p <                        0.001, η2      = 0.40).
Unsurprisingly, human accuracy was lowest in p
Credit dataset, for which AI accuracy is                        the German
and human accuracy was highest                            also relatively low,
                                                          in the Census dataset,
where AI accuracy is also relatively high.
To assess confidence calibration,                         we fit a multilevel
model (Gelman & Hill, 2006) that included dataset, AI
condition, and confidence ratings as fixed-effect predictors,

with varying intercepts for each participant. The multilevel
model accounts for a moderate amount of variance in fixed
and varying effects, conditional Pseudo-R2 = 0.10. This
model indicated that the relationship between confidence
and accuracy interacted with both dataset and AI condition,
illustrated in Figure 4.
As hypothesized, confidence was most highly calibrated
with accuracy in the AI Uncertainty condition, followed
by the AI Prediction condition, and lowest in the Control
condition. See data and code links in the supplementary
material for details of the model.
We also analyzed the impact of AI condition and dataset
upon participants’ response times (RT). Using an omnibus
3 x 3 ANOVA, we found a significant main effect of dataset
upon RT (F (2, 436)         = 6.22, p                     = 0.002, η2    = 0.03),
with participants responding slowest in the                      p
                                                                 Student Per-
formance dataset (see left side of Figure 5), perhaps due
 4000      Dataset
           Credit
           Student
           +Census
           Control  AI Prediction Al Uncertainty    Point  Dist    Point  Dist
                    Condition                              Condition

Figure 5. Participant response times (RT) in milliseconds in Exper-
iment 1 (left) and Experiment 2 (right). Error bars represent 95%
confidence intervals.


to the larger number of attributes to consider for each in-
stance. We did not find significant effects of AI condition
(F (2, 436) = 1.97, p = 0.14, η2 = 0.009) or an interaction
effect (F (4, 436) = 0.13, p                        p      2
                                                    = 0.97, ηp = 0.001). These
results imply that the accuracy benefit for AI UQ informa-
tion is not merely due to a speed/accuracy tradeoff among
participants (Wickelgren, 1977).

5. Experiment 2
Experiment 1 demonstrated that decision-making perfor-
mance can be improved with AI UQ information; we de-
signed Experiment 2 to test if different representations
of UQ might be more or less beneficial for decision-
making.           We compared performance with distributions
of uncertainty probabilities to point-estimated probabili-
ties, as well as two different visualizations of uncertainty
(needle vs. dotplot), again using the same three datasets
used in Experiment 1 (see Figure 6). All hypotheses and
methods were pre-registered (
                                                    https://aspredicted.
org/CJW_71H).

                                                                                         6



                                            AI UQ to Improve Human Decision-Making

                                Needle                                                                        Dotplot

    Point                                   Distribution                                                Point                         Distribution

    Break Terms  60%  50%  60%  Follow Terms    Break Terms  60%  50%  60%  Follow Terms
    70%                         70%             70%
    80                                 80%

    90%                                   90%    90%                        90%
                                                                                        100% 90% 80% 70% 60% 50% 60% 70%80% 90% 100%  100% 90% 80% 70% 60% 50% 60% 70% 80% 90% 100%
                                                                                         Break Terms             Follow Terms         Break Terms    Follow Terms

    Figure 6. Example of the four conditions (point vs. distribution, and needle vs. dotplot) in Experiment 2, using a trial from the German
    Credit dataset.

    We hypothesized that for all three datasets, both partici-
    pant decision accuracy and confidence calibration would be                            1.0           Credit                        Student        Census
    higher with distribution information than for point-estimated
    UQ in the AI Uncertainty condition, followed by the AI Pre-                          200.8
    diction condition, and lowest in the Control condition. We
    also hypothesized that, within the distribution conditions,                           0.6
    accuracy and confidence calibration would be higher for the
    dotplot visualization than for the needle visualization, due                          0.4
    to the more detailed information about the shape of distribu-
    tions available in the dotplot, compared to the needle which                                 1      2  3      4  5 1         2    3          4   55  1  2  3  4  5
    only shows the distributions as a uniform range.                                                                      Confidence Ratings

5.1. Participants

We recruited 50 participants in each cell, for a total of 600
participants (48.5% male, 49.3% female, 2.2% other or
prefer not to answer), from the Prolific platform.                      Most
participants (75.3%) were 18-44 years old.

5.2. Results and Discussion
We excluded 553 trials (out of 24,000 across all partic-
ipants) with reaction times that exceeded three standard
deviations above the mean. An omnibus 2 (point vs. distri-
bution) x 2 (needle vs. dotplot) x 3 (dataset) ANOVA for
mean accuracy indicated a significant main effect of dataset
(F (2, 588) = 188.77, p < 0.001, η2 = 0.39), where perfor-
mance was again highest for the              p
                                             Census dataset, followed by
Student Performance, and lowest for German Credit. How-
ever, there was no significant main effects of point vs. dis-
tribution (F (1, 588) = 0.28, p = 0.60, η2 < 0.001) or vi-
sualization type (F (1, 588) = 0.16, p                   p         2    ).
Neither was there evidence of                       = 0.69, ηp < 0.001
                                             significant first-order inter-
action effects among the three manipulated variables, see
right side of Figure 3).
As in Experiment 1, we fit multilevel models with varying
intercepts for each participant to assess confidence calibra-
tion. We found that the best-fitting model (as assessed by fit
statistics: AIC and BIC) had only confidence, dataset, and

their interaction as fixed-effect predictors, see Figure 7.


Figure 7. Predicted effects (level 2/overall results of multilevel
 model) of confidence ratings and dataset upon accuracy in Experi-
 ment 2. Steeper, positively-sloped lines indicate better confidence
 calibration. Shaded areas represent 95% confidence intervals for
 the predicted values.
 The best fit model indicates confidence calibration, again,
 accounts for a moderate amount of variance in fixed and
 varying effects, conditional Pseudo-R2 = 0.13. Including
 point vs. distribution or visualization type did not appear to
 improve model fit or to predict accuracy performance above
 and beyond what is predicted by dataset and confidence.
 See data and code links in the supplementary material for
 details of the model.
 Thus, contrary to the pre-registration, neither our accuracy

 or confidence calibration results indicate support for our
 Experiment 2 hypotheses. The hypotheses were that perfor-
 mance would be better with distribution information for UQ
 than for point-estimated UQ, and that within the distribution

 condition, performance would be better with dotplots than
 with the needle visualization.
 Additionally, we analyzed RT in Experiment 2 (see right
 side of Figure 5) to rule out speed/accuracy tradeoffs in
 the performance results. An omnibus 2 x 2 x 3 ANOVA
 indicated no significant effect of dataset (F (2, 588)                      =
 0.09, p        = 0.92, η2     < 0.001), point vs.                     distribution
 (F (1, 588) =               p                   2
                          0.06, p = 0.81, ηp             < 0.001), or visualiza-
 tion type (F (1, 588) = 0.01, p = 0.91, η2 < 0.001) upon
 RT. Neither was there evidence                                  p
                                                 of significant interaction

                                                                                        7



                                         AI UQ to Improve Human Decision-Making

effects among the three manipulated variables.

5.3. Exploratory Analyses
Although this work was not specifically designed to as-
sess whether the combination of humans plus AI generally
exceeded the accuracy of AI predictions, we conducted ex-
ploratory analyses to investigate how often this occurred. In
both experiments, we found that a small subset of partici-
pants were able to outperform the accuracy of the AI pre-
dictions they received (see Table 1). AI uncertainty enabled
humans to outperform the AI accuracy more frequently and
with patterns of mean differences suggesting greater im-
provements.


Table 1. Number of participants who outperformed the AI

 Human Accuracy             >AI (n)  ≤AI (n)  >AI (%)
 Experiment 1
  AI Prediction              2       148          1.33%
  AI Uncertainty             7       141          4.73%
 Experiment 2
  Point Needle               8       142          5.33%
  Point Dotplot              8       142          5.33%
  Dist Needle                11      139          7.33%
  Dist Dotplot               7       143          4.67%


6. General Discussion, Limitations, and Future
Work

The overall results of Experiment 1 showed that providing
AI UQ to human decision-makers improved accuracy and
confidence calibration performance over and above provid-
ing an AI prediction alone. In Experiment 2, we did not find
meaningful differences between different representations of
UQ. Taken together, our findings suggest that the benefit of
AI UQ may not be overly sensitive to the representation of
the UQ information. Also, adding more information (here,
the UQ distribution), even though task-relevant, did not
improve decision-making which is consistent with prior re-
search (Marusich et al., 2016; Gigerenzer & Brighton, 2009;
Alufaisan et al., 2021).
Here, the humans and AI had identical information so we
could evaluate well-calibrated UQ. Normatively, algorithms
tend to outperform human decision-making in a variety of
tasks; a clear exception is when people have knowledge the
algorithm does not (Dawes et al., 1989). In such situations,
it is possible that the AI and human combined can produce
better performance than either alone (Cummings, 2014).
Exploratory results suggested AI uncertainty may increase
the frequency of people exceeding AI accuracy, although
this was not a common occurrence; likely because both the
human and the AI had the same information.
In this work, we did not compare different UQ techniques
but did demonstrate that, using a Brier score, our UQ tech-
nique performed well in practice. It is possible that UQ tech-
niques that do not perform as well as ours may not improve
human decision-making accuracy. In future work, we plan
to use other UQ techniques, including less effective ones
to understand the impact of UQ quality on human decision-
making accuracy. Similarly, behavioral experiments were
limited to comparing point versus distributions and two vi-
sualizations of predictive uncertainty. It is possible that
other visualizations may make providing distribution infor-
mation more effective, and we plan to conduct future work
assessing more UQ visualization techniques.
Finally, we used a relatively simple binary classification
decision-making task. In more complex application do-
mains, where humans encounter multi-class classification
problems, the impact of UQ information on human deci-
sion making could be different. Future work should explore
multi-class classification problems, as well as the use of AI
UQ for complex tasks where people, such as experts, have
knowledge unknown to the AI model.

7. Conclusion


Our extensive behavioral experiments show that providing
high quality AI uncertainty information improves human
decision accuracy and confidence calibration over the AI
prediction alone. This human performance benefit was not
limited to only a specific visual representation of UQ infor-
mation. In previous work, there is an absence of evaluating
calibration for AI UQ (Lai et al., 2021). Here, we used Brier
scores to quantify the calibration of our implementation of
AI UQ. We showed the AI UQ used here was well-calibrated
by leveraging the existing class labels of test instances.

8. Impact Statements
Recently, understanding how humans and AI systems can
work better together has emerged as an important challenge.
Although previous work has explored how and when ex-
plainable AI may help human decision making, the impact
of providing uncertainty information to humans has not
been explored in depth in the context of AI systems. To
address this challenge, in this work, we explore whether
providing uncertainty information to humans may improve
decision making and potentially correct errors caused by the
AI models.

Software and Data

See supplementary material or https://osf.io/
cb762/.



                                                                     8



                                       AI UQ to Improve Human Decision-Making

    Acknowledgements                                                     1145/3461702.3462571.   URL https://doi.org/
       We thank Mary Grace Kozuch for helpful feedback on a               10.1145/3461702.3462571.
    previous version of this paper.                                      Buc
                                                                          ¸inca, Z., Malaya, M. B., and Gajos, K. Z. To trust or
     The views and conclusions contained in this document are             to think: Cognitive forcing functions can reduce over-
    those of the authors and should not be interpreted as rep-            reliance on ai in ai-assisted decision-making.  Proc.
    resenting the official policies, either expressed or implied          ACM Hum.-Comput. Interact., 5(CSCW1), apr 2021.
    of the U.S. DEVCOM Army Research Laboratory or the                    doi: 10.1145/3449287.  URL https://doi-org.
    U.S. Government.  The U.S. Government is authorized                   ezproxy.uta.edu/10.1145/3449287.
    to reproduce and distribute reprints for Government pur-             Chen, C., Ding, N., and Carin, L. On the convergence
    poses notwithstanding any copyright notation. M.K. and                of stochastic gradient mcmc algorithms with high-order
              Y.Z. were supported in part by NSF awards OAC-1828467,      integrators.  In Proceedings of the 28th International
    DMS-1925346, CNS2029661, OAC-2115094, ARO award                       Conference on Neural Information Processing Systems
    W911NF-17-1-0356, and a gift from Cisco Inc.                               - Volume 2, NIPS’15, pp. 2278–2286, Cambridge, MA,

    References                                                            USA, 2015. MIT Press.
    Abdar, M., Pourpanah, F., Hussain, S., Rezazadegan, D.,              Chen, T., Fox, E., and Guestrin, C.      Stochastic gra-
     Liu, L., Ghavamzadeh, M., Fieguth, P., Cao, X., Khosravi,            dient hamiltonian monte carlo.   In Xing, E. P. and
     A., Acharya, U. R., Makarenkov, V., and Nahavandi, S.                Jebara, T. (eds.),     Proceedings of the 31st Interna-
     A review of uncertainty quantification in deep learning:             tional Conference on Machine Learning, volume 32
     Techniques, applications and challenges. Inf. Fusion, 76             of  Proceedings of Machine Learning Research,   pp.
     (C):243–297, dec 2021a. ISSN 1566-2535. doi: 10.1016/                        1683–1691, Bejing, China, 22–24 Jun 2014. PMLR.
     j.inffus.2021.05.008. URL https://doi.org/10.                        URL https://proceedings.mlr.press/v32/
     1016/j.inffus.2021.05.008.                                           cheni14.html.

Abdar, M., Pourpanah, F., Hussain, S., Rezazadegan, D.,
 Liu, L., Ghavamzadeh, M., Fieguth, P., Cao, X., Khos-
 ravi, A., Acharya, U. R., et al. A review of uncertainty
 quantification in deep learning: Techniques, applications
 and challenges. Information Fusion, 76:243–297, 2021b.
Cokely, E. T., Galesic, M., Schulz, E., Ghazal, S., and
 Garcia-Retamero, R. Measuring risk literacy: The berlin
 numeracy test.      Judgment and Decision making, 7(1):
 25–47, 2012.
Cummings, M. M. Man versus machine or man+ machine?

    Alufaisan, Y., Marusich, L. R., Bakdash, J. Z., Zhou,                 IEEE Intelligent Systems, 29(5):62–69, 2014.
     Y., and Kantarcioglu, M.          Does explainable artificial
     intelligence improve human decision-making?    Pro-                 Dawes, R. M., Faust, D., and Meehl, P. E.  Clinical ver-
     ceedings of the AAAI Conference on Artificial Intelli-               sus actuarial judgment. Science, 243(4899):1668–1674,
     gence, 35(8):6618–6626, May 2021.        doi:  10.1609/              1989.
     aaai.v35i8.16819. URL https://ojs.aaai.org/                         De Leeuw, J. R. jspsych: A javascript library for creat-
     index.php/AAAI/article/view/16819.                                   ing behavioral experiments in a web browser. Behavior

    Amershi, S., Weld, D., Vorvoreanu, M., Fourney, A., Nushi,            research methods, 47:1–12, 2015.
     B., Collisson, P., Suh, J., Iqbal, S., Bennett, P. N., Inkpen,
     K., et al. Guidelines for human-ai interaction. In Pro-             Ding, N., Fang, Y., Babbush, R., Chen, C., Skeel, R. D.,
     ceedings of the 2019 CHI conference on human factors                 and Neven, H.    Bayesian sampling using stochastic
     in computing systems, pp. 1–13, 2019.                                gradient thermostats.  In Ghahramani, Z., Welling,
                                                                          M.,  Cortes,  C., Lawrence, N.,  and Weinberger, K.
    Bhatt, U.,    ´
                     Antoran, J., Zhang, Y., Liao, Q. V., Sattigeri,      (eds.),  Advances  in  Neural   Information Process-
     P., Fogliato, R., Melanc
                           ¸on, G., Krishnan, R., Stanley,                ing  Systems,    volume 27. Curran    Associates, Inc.,
                 J., Tickoo, O., Nachman, L., Chunara, R., Srikumar,      2014.    URL https://proceedings.neurips.
     M., Weller, A., and Xiang, A.     Uncertainty as a form              cc/paper_files/paper/2014/file/
     of transparency:              Measuring, communicating, and us-      21fe5b8ba755eeaece7a450849876228-Paper.
     ing uncertainty. In Proceedings of the 2021 AAAI/ACM                 pdf.
     Conference on AI, Ethics, and Society, AIES ’21, pp.
     401–413, New York, NY, USA, 2021. Association for                   Dua, D. and Graff, C. UCI machine learning repository,
     Computing Machinery. ISBN 9781450384735. doi: 10.                    2017. URL http://archive.ics.uci.edu/ml.

                                                                        9



                                                AI UQ to Improve Human Decision-Making

    Dwaracherla, V., Wen, Z., Osband, I., Lu, X., Asghari, S. M.,         Hullman, J., Qiao, X., Correll, M., Kale, A., and Kay, M.
      and Roy, B. V.           Ensembles for uncertainty estimation:       In pursuit of error: A survey of uncertainty visualiza-
      Benefits of prior functions and bootstrapping. Transac-              tion evaluation. IEEE transactions on visualization and
      tions on Machine Learning Research, 2023. ISSN 2835-                 computer graphics, 25(1):903–913, 2018.
      8856. URL https://openreview.net/forum?                             Jain, S., Liu, G., Mueller, J., and Gifford, D.         Maxi-
      id=IqJsyulDUX.                                                       mizing overall diversity for improved uncertainty es-

    Farquhar,  S.,    Smith, L., and Gal, Y.           Try depth in-       timates  in deep  ensembles.  Proceedings         of the
      stead of weight correlations:   Mean-field is a less                 AAAI Conference on Artificial Intelligence,       34(04):
      restrictive assumption for deeper networks.    CoRR,                 4264–4271,  Apr. 2020.    doi:  10.1609/aaai.v34i04.
      abs/2002.03704, 2020. URL https://arxiv.org/                         5849.    URL https://ojs.aaai.org/index.
      abs/2002.03704.                                                      php/AAAI/article/view/5849.

    Frick, J. and Hegg, C. Can end-users’ flood management de-            Jalaian, B., Lee, M., and Russell, S. Uncertain context: Un-
      cision making be improved by information about forecast              certainty quantification in machine learning. AI Magazine,
      uncertainty? Atmospheric Research, 100(2-3):296–303,                 40(4):40–49, 2019.
      2011.                                                               Jiang, H., Kim, B., Guan, M. Y., and Gupta, M. To trust or

Gelman, A. and Hill, J. Data analysis using regression and
  multilevel/hierarchical models. Cambridge University
  Press, 2006.

Ghosh, P., Sajjadi, M. S. M., Vergari, A., Black, M. J.,
  and Sch ¨
           olkopf, B. From variational to deterministic au-
  toencoders. In 8th International Conference on Learn-
  ing Representations, ICLR 2020, Addis Ababa, Ethiopia,
 April 26-30, 2020. OpenReview.net, 2020. URL https:
 //openreview.net/forum?id=S1g7tpEYDS.
 not to trust a classifier. In Proceedings of the 32nd Inter-
 national Conference on Neural Information Processing
 Systems, NIPS’18, pp. 5546–5557, Red Hook, NY, USA,
 2018. Curran Associates Inc.
Joslyn, S. and LeClerc, J. Decisions with uncertainty: The

 glass half full. Current Directions in Psychological Sci-
 ence, 22(4):308–315, 2013.

Kendall,  A. and Gal,  Y.  What uncertainties do we
 need in bayesian deep learning for computer vision?

Gigerenzer, G. and Brighton, H. Homo heuristicus: Why
  biased minds make better inferences. Topics in cognitive
  science, 1(1):107–143, 2009.

Gigerenzer, G., Gaissmaier, W., Kurz-Milcke, E., Schwartz,
  L. M., and Woloshin, S. Helping doctors and patients
  make sense of health statistics. Psychological science in
  the public interest, 8(2):53–96, 2007.
In Guyon, I., Luxburg, U. V., Bengio, S., Wallach,
H., Fergus, R.,  Vishwanathan,  S., and Garnett,     R.
(eds.),  Advances in  Neural    Information       Process-
ing Systems, volume       30. Curran Associates,     Inc.,
2017.    URL https://proceedings.neurips.
cc/paper_files/paper/2017/file/
2650d6089a6d640c5e85b2b88265dc2b-Paper.
pdf.

Gneiting, T. and Raftery, A. E.   Strictly proper scoring
  rules, prediction, and estimation. Journal of the American
  Statistical Association, 102(477):359–378, 2007. doi:
 10.1198/016214506000001437. URL https://doi.
  org/10.1198/016214506000001437.

Gong, W., Tschiatschek, S., Nowozin, S., Turner, R. E.,
  Hern´
          andez-Lobato, J. M., and Zhang, C. Icebreaker:
  Element-wise efficient information acquisition with a
  bayesian deep latent gaussian model.      In Wallach, H.,
  Larochelle, H., Beygelzimer, A., d'Alche-Buc, F., Fox, E.,
                                            ´
  and Garnett, R. (eds.), Advances in Neural Information
  Processing Systems, volume 32. Curran Associates, Inc.,
  2019.    URL https://proceedings.neurips.
  cc/paper_files/paper/2019/file/
  c055dcc749c2632fd4dd806301f05ba6-Paper.
  pdf.
Lahlou, S., Jain, M., Nekoei, H., Butoi, V. I., Bertin, P.,
 Rector-Brooks, J., Korablyov, M., and Bengio, Y. DEUP:
 Direct epistemic uncertainty prediction. Transactions
 on Machine Learning Research, 2023.      ISSN 2835-
 8856. URL https://openreview.net/forum?
 id=eGLdVRvvfQ.
Lai, V., Chen, C., Liao, Q. V., Smith-Renner, A., and Tan, C.

 Towards a science of human-ai decision making: a survey
 of empirical studies.       arXiv preprint arXiv:2112.11471,
 2021.


Lakshminarayanan, B., Pritzel, A., and Blundell, C. Sim-
 ple and scalable predictive uncertainty estimation using
 deep ensembles. In Proceedings of the 31st International
         Conference on Neural Information Processing Systems,
 NIPS’17, pp. 6405–6416, Red Hook, NY, USA, 2017.
 Curran Associates Inc. ISBN 9781510860964.

                                                                        10



                                        AI UQ to Improve Human Decision-Making

    Li, C., Stevens, A., Chen, C., Pu, Y., Gan, Z., and Carin,               Oberdiek, P., Fink, G. A., and Rottmann, M. UQGAN: A
     L.  Learning weight uncertainty with stochastic gradi-                   unified model for uncertainty quantification of deep classi-
     ent mcmc for shape classification. In Proceedings of                     fiers trained via conditional GANs. In Oh, A. H., Agarwal,
     the IEEE Computer Society Conference on Computer                         A., Belgrave, D., and Cho, K. (eds.), Advances in Neural
     Vision and Pattern Recognition, pp. 5666–5675. IEEE                      Information Processing Systems, 2022.  URL https:
     Computer Societyhelp@computer.org, December 2016.                       //openreview.net/forum?id=djOANbV2zSu.
     ISBN 9781467388504. doi: 10.1109/CVPR.2016.611.                         Prabhudesai, S., Yang, L., Asthana, S., Huan, X., Liao,
     Generated from Scopus record by KAUST IRTS on 2021-                      Q. V., and Banovic, N. Understanding uncertainty: How
     02-09.                                                                   lay decision-makers perceive and interpret uncertainty in

    Liu, H., Ji, R., Li, J., Zhang, B., Gao, Y., Wu, Y., and Huang,           human-ai decision making. In Proceedings of the 28th
     F.  Universal adversarial perturbation via prior driven                  International Conference on Intelligent User Interfaces,
     uncertainty approximation. In 2019 IEEE/CVF Interna-                     IUI ’23, pp. 379–396, New York, NY, USA, 2023. Asso-
     tional Conference on Computer Vision, ICCV 2019, Seoul,                  ciation for Computing Machinery. ISBN 9798400701061.
     Korea (South), October 27 - November 2, 2019, pp. 2941–                  doi: 10.1145/3581641.3584033. URL https://doi.
     2949. IEEE, 2019a.   doi:                 10.1109/ICCV.2019.00303.       org/10.1145/3581641.3584033.
     URL https://doi.org/10.1109/ICCV.2019.                                  Salakhutdinov, R. and Mnih, A. Bayesian probabilistic
     00303.                                                                   matrix factorization using markov chain monte carlo.

Liu, J., Paisley, J., Kioumourtzoglou, M.-A., and Coull,
 B. Accurate uncertainty estimation and decomposition
 in ensemble learning.  In Wallach, H., Larochelle,
 H., Beygelzimer,          ´
 Garnett, R. (eds.),A., d'Alche-Buc, F., Fox, E., and
 Processing           Advances in Neural Information
               Systems, volume 32. Curran Associates, Inc.,
 2019b.    URL https://proceedings.neurips.
 cc/paper_files/paper/2019/file/
 1cc8a8ea51cd0adddf5dab504a285915-Paper.
 pdf.
 In Proceedings of the 25th International Conference
 on Machine Learning, ICML ’08, pp. 880–887, New
 York, NY, USA, 2008. Association for Computing
 Machinery.  ISBN 9781605582054.        doi:         10.1145/
 1390156.1390267.  URL https://doi.org/10.
 1145/1390156.1390267.

Salimans, T., Kingma, D., and Welling, M. Markov chain
 monte carlo and variational inference: Bridging the
 gap.  In Bach, F. and Blei, D. (eds.), Proceedings of
 the 32nd International Conference on Machine Learning,

    Louizos, C. and Welling, M. Multiplicative normalizing                          volume 37 of Proceedings of Machine Learning Research,
     flows for variational bayesian neural networks. In Pro-                  pp. 1218–1226, Lille, France, 07–09 Jul 2015. PMLR.
     ceedings of the 34th International Conference on Ma-                     URL https://proceedings.mlr.press/v37/
     chine Learning - Volume 70, ICML’17, pp. 2218–2227.                      salimans15.html.
     JMLR.org, 2017.                                                         Schemmer, M., Hemmer, P., Nitsche, M., K ¨
                                                                                                                     uhl, N., and
                                                                              Vossing, M. A meta-analysis of the utility of explainable
    Marusich, L. R., Bakdash, J. Z., Onal, E., Yu, M. S., Schaf-              ¨
     fer, J., O’Donovan, J., Hollerer, T., Buchler, N., and Gon-              artificial intelligence in human-ai decision-making. In
     zalez, C.              ¨                                                 Proceedings of the 2022 AAAI/ACM Conference on AI,
                   Effects of information availability on command-            Ethics, and Society, pp. 617–626, 2022.
     and-control decision making: Performance, trust, and sit-
     uation awareness. Human Factors, 58(2):301–321, 2016.                   Sensoy, M., Kaplan, L., and Kandemir, M. Evidential deep
     doi:    10.1177/0018720815619515.                     URL https://       learning to quantify classification uncertainty. In Proceed-
     doi.org/10.1177/0018720815619515. PMID:                                  ings of the 32nd International Conference on Neural In-
     26822796.                                                                formation Processing Systems, NIPS’18, pp. 3183–3193,

Morss, R. E., Demuth, J. L., and Lazo, J. K. Communicat-
 ing uncertainty in weather forecasts: A survey of the us
 public.   Weather and forecasting, 23(5):974–991, 2008.

Nadav-Greenberg, L. and Joslyn, S. L. Uncertainty forecasts
 improve decision making among nonexperts. Journal of
 Cognitive Engineering and Decision Making, 3(3):209–
 227, 2009.
 Red Hook, NY, USA, 2018. Curran Associates Inc.
Subedar, M., Krishnan, R., Meyer, P. L., Tickoo, O., and
 Huang, J. Uncertainty-aware audiovisual activity recog-
 nition using deep bayesian variational inference. In Pro-
 ceedings of the IEEE/CVF International Conference on
 Computer Vision (ICCV), October 2019.
Vasconcelos, F., He, B., Singh, N. M., and Teh, Y. W.
 UncertaINR: Uncertainty quantification of end-to-end

    Neal, R. M. Bayesian learning for neural networks, volume                 implicit neural representations for computed tomogra-
    118. Springer Science & Business Media, 2012.                             phy. Transactions on Machine Learning Research, 2023.

                                                                           11



                               AI UQ to Improve Human Decision-Making

 ISSN 2835-8856.      URL https://openreview.
 net/forum?id=jdGMBgYvfX.
Wang, G., Li, W., Aertsen, M., Deprest, J., Ourselin, S., and
 Vercauteren, T. Aleatoric uncertainty estimation with test-
 time augmentation for medical image segmentation with
 convolutional neural networks. NEUROCOMPUTING,
 338:34–45, April 2019. ISSN 0925-2312. doi: 10.1016/j.
 neucom.2019.01.103.
Wickelgren, W. A. Speed-accuracy tradeoff and information
 processing dynamics. Acta Psychologica, 41(1):67–85,
 1977.
Wilson, A. G. and Izmailov, P.   Bayesian deep learning
 and a probabilistic perspective of generalization.  In
 Proceedings of the 34th International Conference on
 Neural Information Processing Systems, NIPS’20, Red
 Hook, NY, USA, 2020. Curran Associates Inc.       ISBN
 9781713829546.
Zhang, Y., Liao, Q. V., and Bellamy, R. K. Effect of confi-
 dence and explanation on accuracy and trust calibration in
 ai-assisted decision making. In Proceedings of the 2020
 conference on fairness, accountability, and transparency,
 pp. 295–305, 2020.

12