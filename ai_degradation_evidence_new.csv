Source,Industry Domain/Field,AI Features,Performance Degradation Types,Causal Links,Excerpt,Justification,Validation
2309.10852v2,artificial intelligence in decision-making,Overconfidence in AI predictions,Over-reliance on AI predictions,"Overconfidence in AI predictions can lead to over-reliance on AI, as users may trust AI outputs without sufficient skepticism, potentially degrading decision quality.","""However, prediction probabilities are prone to overconfidence in some AI models. There is a lack of discussion on the calibration of uncertainty estimates in the existing literature."" (p. 3)","This excerpt indicates that overconfidence in AI predictions can lead to over-reliance on AI, as users may trust AI outputs without sufficient skepticism, potentially degrading decision quality.",y
2309.10852v2,decision science,AI Uncertainty Quantification (UQ),Improved decision-making accuracy,"AI Uncertainty Quantification (UQ) improves decision-making accuracy by providing well-calibrated probabilistic information, enhancing users' ability to make informed decisions.","""Our results indicate well-calibrated AI UQ is beneficial for decision-making."" (p. 1)","This excerpt shows that AI UQ improves decision-making accuracy by providing well-calibrated probabilistic information, enhancing users' ability to make informed decisions.",y
2309.10852v2,cognitive psychology,Excessive information,Decision-making performance not always improved,"Providing excessive information, even if task-relevant, does not always improve decision-making performance, suggesting a potential overload effect.","""However, increasing information, even when it is task-relevant, is not always beneficial to human decision-making performance."" (p. 2)","This excerpt suggests that providing excessive information, even if task-relevant, does not always improve decision-making performance, indicating a potential overload effect.",y
2309.10852v2,decision support systems,AI prediction alone,Lower decision accuracy,"AI prediction alone may lead to lower decision accuracy compared to when AI Uncertainty Quantification is provided, as users lack additional probabilistic context.","""Our results showed that UQ was beneficial for decision-making performance compared to only AI predictions."" (p. 1)","This excerpt indicates that AI prediction alone may lead to lower decision accuracy compared to when AI UQ is provided, as users lack additional probabilistic context.",y
2309.10852v2,Decision Science,Visual representation of UQ,No significant impact on performance,"Visual representation of UQ does not significantly impact decision-making performance, suggesting that the format of UQ presentation may not be critical.","""In Experiment 2, we did not find meaningful differences between different representations of UQ."" (p. 7)","This excerpt suggests that visual representation of UQ does not significantly impact decision-making performance, indicating that the format of UQ presentation may not be critical.",y
41746_2025_Article_1559,healthcare,High-level automation,Complacency bias,High-level automation in AI-enabled decision-support tools can lead to complacency bias because users may overly rely on AI systems and reduce their vigilance in monitoring tasks.,"""It is therefore critical that these AI tools are developed in a manner that avoids automation bias, complacency bias and deskilling9,35."" (p. 5)","The excerpt identifies complacency bias as a risk associated with AI tools, suggesting that users may become overly reliant on AI, thus reducing their active engagement and vigilance.",y
41746_2025_Article_1559,healthcare,AI-enabled decision-support tools,Deskilling,"AI-enabled decision-support tools can cause deskilling because they take over tasks traditionally performed by humans, leading to a reduction in skill use and development.","""It is therefore critical that these AI tools are developed in a manner that avoids automation bias, complacency bias and deskilling9,35."" (p. 5)","The excerpt highlights deskilling as a potential consequence of AI tools, implying that reliance on AI could lead to a decline in human skills as tasks are automated.",y
41746_2025_Article_1559,medical imaging,AI-based image analysis,Stress and increased workload,"AI-based image analysis systems can increase stress and workload because they may reject images that humans could process, leading to additional tasks for staff.","""The authors identified a high image-quality related rejection rate of images that human readers could screen, with resultant increased staff workload, elevated stress, and longer patient waiting times."" (p. 5)","This excerpt shows that AI systems can increase workload and stress by rejecting images that humans would accept, thus creating additional work and stress for human operators.",y
730709,digital media and content recommendation,Recommendation algorithms,Reduced serendipity,"Recommendation algorithms limit serendipity by reinforcing past behaviors, which restricts exploration and change, thereby constraining human agency.","""However, recommendation algorithms often limit our experience of serendipity. They do so because most of these systems are set up to feed content based on our past behaviors (André et al. 2018; Wertenbroch et al. 2020). Consequently, they reinforce those behaviors and create inertia that limits exploration and change (Talaifar and Lowery 2023), constraining human agency."" (p. 244)","This excerpt shows how recommendation algorithms reinforce existing behaviors, limiting new experiences and exploration, which constrains human agency.",y
730709,knowledge work and AI integration,Generative AI models,De-skilling,"Generative AI models lead to de-skilling by performing tasks that humans used to do, reducing opportunities for skill practice and strengthening.","""The emergence of ChatGPT and other forms of generative AI have revived the concern that AI could contribute to a new wave of de-skilling among knowledge workers. For example, generative AI models can now outperform most humans at creative idea generation (Guzik, Byrge, and Gilde 2023; Koivisto and Grassini 2023). Relying on such models can increase the quantity and quality of knowledge workers’ output (Noy and Zhang 2023; Dell’Acqua et al. 2024). If workers begin to rely on AI to perform parts of their jobs, such as writing or brainstorming, might they be foregoing opportunities to practice and strengthen those skills?"" (p. 244)","This excerpt illustrates how reliance on AI for tasks like writing or brainstorming can lead to skill atrophy, as humans forego opportunities to practice these skills.",y
730709,social media technology,AI in social media,Reduced emotional intelligence,"AI in social media leads to reduced emotional intelligence by increasing reliance on technology for social interactions, which affects understanding and navigation of social cues.","""For instance, recent research suggested that, as consumers trade off human contact with more reliance on AI-powered technology, there may be an increased variance in social adjustment, impacting emotional intelligence, particularly among our youth (e.g., Beranuy et al. 2009; Ralph and Nunez 2021)."" (p. 244)",This excerpt highlights how increased reliance on AI for social interactions can impact emotional intelligence by affecting social adjustment and understanding of social cues.,y
730709,human-computer interaction,AI-based recommendation systems,Automation bias,"AI-based recommendation systems can lead to automation bias by presenting content that aligns with past behaviors, which influences decision-making and reduces self-determination.","""Because AI systems can shape our environment—what we see and the opportunities available to us (Grafanaki 2017)—they can subtly manipulate decision-making trajectories in ways that ultimately constrain self-determination (Bhattacharjee et al. 2014)."" (p. 243)","This excerpt demonstrates how AI systems influence decision-making by presenting content that aligns with past behaviors, leading to automation bias and reduced self-determination.",y
730709,Human-Computer Interaction,AI systems,Skill decay,"AI systems contribute to skill decay by taking over tasks that humans previously performed, leading to atrophy of those skills over time.","""As new technologies emerge and mature, they often take over certain tasks or skills that were previously performed by humans. Off-loading tasks or skills to technology can mean that those skills atrophy over time, a phenomenon known as de-skilling (Wood 1987)."" (p. 244)","This excerpt explains how AI systems taking over tasks can lead to skill decay, as humans no longer practice those skills, causing them to atrophy.",y
730709,human-computer interaction,AI systems,Reduced self-expression,"AI systems reduce self-expression by requiring users to adapt their communication to fit the system's limitations, which can lead to less authentic interactions.","""These reductions can lead to feelings of being diminished or 'less than' because the consumer must interact in ways that do not fully represent their potential or preferred modes of interaction."" (p. 243)","This excerpt shows how AI systems constrain self-expression by forcing users to adapt their communication, leading to less authentic interactions.",y
Article-4-JDAH-41,Artificial Intelligence Ethics,Lack of transparency and interpretability,Skill decay,"Black-box AI models lead to skill decay because users cannot understand or interpret the decision-making process, reducing their ability to engage critically with the system.","""The black box problem in AI refers to the difficulty in understanding how AI systems and machine learning models process data and generate predictions or decisions."" (p. 33)","This excerpt shows that the lack of transparency in AI models prevents users from understanding the system, which can lead to a decline in their critical engagement and skills.",y
Article-4-JDAH-41,AI governance and regulation,Black-box models,Complacency,"Black-box models lead to complacency because users may overly rely on AI outputs without understanding the underlying processes, reducing their vigilance.","""The inability to effectively monitor and regulate AI systems has already strained relationships between different industries and regulatory bodies."" (p. 33)","This excerpt indicates that the lack of interpretability in black-box models can lead to a reduction in user vigilance, as they cannot effectively monitor or regulate the system.",y
Article-4-JDAH-41,Artificial Intelligence Ethics,Black-box models,Automation bias,Black-box models lead to automation bias because users may trust AI outputs without questioning them due to the lack of transparency.,"""The lack of transparency and interpretability of black-box models can lead to serious mistakes, and even dangerous decisions."" (p. 33)","This excerpt suggests that the opacity of black-box models can cause users to trust AI outputs blindly, leading to potential errors.",y
Human-AI interaction research agenda_ A user-centered perspective - ScienceDirect,autonomous vehicles,High-level automation in autonomous vehicles,Low trust and acceptance,High-level automation in autonomous vehicles leads to low trust and acceptance because users are not comfortable with the lack of human control and potential safety issues.,"""For lack of trust, however, public acceptance of and consumer readiness for autonomous vehicles remain low at present (Alawadhi et al., 2020).""","This excerpt shows a causal link as it directly mentions the lack of trust as a reason for low public acceptance of autonomous vehicles, highlighting a performance degradation in terms of user acceptance.",y
Human-AI interaction research agenda_ A user-centered perspective - ScienceDirect,Human-AI Interaction,AI replacing human jobs,Resistance and negative attitudes towards AI,AI replacing human jobs leads to resistance and negative attitudes because people view AI as a threat to their employment and human uniqueness.,"""More and more people have viewed AI as job competitors to humans and as a potential threat to human uniqueness and control over the world.""","This excerpt indicates a causal relationship where AI's role in replacing jobs causes resistance and negative attitudes, representing a degradation in human acceptance and cooperation with AI.",y
Human-AI interaction research agenda_ A user-centered perspective - ScienceDirect,Human-Computer Interaction,Opaque decision-making in AI systems,Low trust and acceptance,"Opaque decision-making in AI systems leads to low trust and acceptance because users cannot understand or verify AI decisions, causing discomfort and skepticism.","""A fundamental obstacle lies in the fact that many AI algorithms operate as black boxes, making it difficult for humans to understand the reasoning behind their decisions.""",This excerpt shows a causal link as it explains how the lack of transparency in AI decision-making processes results in low trust and acceptance from users.,y
Human-AI interaction research agenda_ A user-centered perspective - ScienceDirect,Human-Computer Interaction,Anthropomorphic design in AI,Uncanny valley effect,Anthropomorphic design in AI leads to the uncanny valley effect because slight imperfections in human-like AI can cause discomfort and eeriness among users.,"""The uncanny valley hypothesis includes three stages demonstrating different patterns of change in users’ emotional response...when the resemblance reaches a certain level but there still exist subtle imperfections, such anomalies can be detected by users and evoke a feeling of eeriness or discomfort, known as the 'valley'.""","This excerpt shows a causal link by explaining how anthropomorphic design can lead to the uncanny valley effect, which is a form of performance degradation in user experience.",y
TowardsContext-AwareModelingofSituationAwarenessinConditionallyAutomatedDriving,autonomous vehicles,Conditionally automated driving,Decreased vigilance and reduced monitoring,"Conditionally automated driving can lead to decreased vigilance and reduced monitoring, resulting in impaired situation awareness and increased accident risk.","""Automation complacency: Overreliance on automation can lead to decreased vigilance and reduced monitoring of the driving environment, resulting in impaired SA and increased accident risk"" (p. 5).",This excerpt shows a causal link between conditionally automated driving and decreased vigilance because it directly states that overreliance on automation leads to reduced monitoring and impaired situation awareness.,y
TowardsContext-AwareModelingofSituationAwarenessinConditionallyAutomatedDriving,autonomous vehicles,Automation errors,Failures in takeovers,"Automation errors can lead to failures in takeovers because they affect the driver's trust and vigilance levels, impacting their ability to respond effectively.","""However, driver engagement in NDRTs can lead to degradation of SA, leading to failures in takeovers"" (p. 3).","This excerpt indicates that automation errors contribute to failures in takeovers by degrading situation awareness, which is crucial for effective responses.",y
TowardsContext-AwareModelingofSituationAwarenessinConditionallyAutomatedDriving,autonomous vehicles,Lack of transparency,Uncertainty and distrust,"Lack of transparency in automated systems can lead to uncertainty and distrust, negatively affecting risk perception and situation awareness.","""A lack of transparency can lead to uncertainty and distrust, negatively affecting risk perception and SA"" (p. 6).","This excerpt shows a causal link between lack of transparency and human performance degradation by explaining how it leads to uncertainty and distrust, impacting situation awareness.",y
TowardsContext-AwareModelingofSituationAwarenessinConditionallyAutomatedDriving,autonomous vehicles,High-level automation,Mode confusion,"High-level automation can cause mode confusion, where drivers struggle to understand the operational mode, leading to inappropriate actions and delayed responses.","""Mode confusion: Drivers may struggle to understand the current operational mode (manual vs. automated) and their respective roles and responsibilities. This uncertainty can lead to inappropriate actions and delayed responses in critical situations"" (p. 5).","This excerpt demonstrates a causal link between high-level automation and mode confusion by describing how it causes drivers to misunderstand their roles, leading to performance issues.",y
TowardsContext-AwareModelingofSituationAwarenessinConditionallyAutomatedDriving,autonomous vehicles,Automation errors,Increased vigilance,"Automation errors can increase vigilance because they heighten drivers' alertness and distrust, leading to enhanced situation awareness.","""Automation errors (i.e., false alarms and misses) further elevated SA by increasing drivers’ distrust and alertness"" (p. 28).","This excerpt indicates a causal relationship where automation errors lead to increased vigilance by making drivers more alert and distrustful, thereby enhancing their situation awareness.",y
amiajnl-2011-000089,human factors engineering,High-level automation,Automation bias,"High-level automation leads to automation bias because users tend to over-rely on automated systems, leading to errors of commission and omission.","""Automation bias (AB)dthe tendency to over-rely on automationdhas been studied in various academic fields."" (p. 121)","This excerpt shows a causal link as it explicitly states the tendency to over-rely on automation, which is a direct cause of automation bias.",y
amiajnl-2011-000089,clinical decision support systems,Lack of explainability,Errors of commission,"Without understanding the reasoning behind automated advice, users may follow incorrect suggestions, leading to errors of commission.","""CDSS...tempt users to reverse a correct decision they have already made, and thus introduce errors of over-reliance."" (p. 121)","This excerpt indicates that the lack of explainability in CDSS can cause users to follow incorrect advice, leading to errors of commission.",y
amiajnl-2011-000089,human factors engineering,Over-trust prompts,Complacency,"Over-trust in automated prompts can lead to complacency, where users do not adequately monitor or verify the automation's output.","""Automation bias and complacency...reflecting the same kind of automation misuse associated with misplaced attention."" (p. 121)","This excerpt links over-trust in automation to complacency, as it mentions misplaced attention due to automation misuse.",y
amiajnl-2011-000089,decision support systems,Opaque decision-making,Skill decay,Opaque decision-making in AI systems can lead to skill decay as users rely more on automation and less on their own expertise.,"""Automation bias can also be found outside of multitask situations, and occurs when there is an active bias toward DSS in decision-making."" (p. 121)","This excerpt suggests that reliance on opaque decision-making systems can lead to a reduction in the use of personal skills, causing skill decay.",y
amiajnl-2011-000089,health informatics,Prominent display of DSS output,Errors of commission,Prominent display of DSS output can lead to errors of commission as users are more likely to follow incorrect advice.,"""Berner et al¹⁸ found that display prominence increased AB, affecting the likelihood of a decision being changed after advice."" (p. 124)",This excerpt shows that the design feature of prominently displaying DSS output can lead to errors of commission by increasing automation bias.,y
fhumd-7-1579166,human-AI collaboration in decision-making systems,High-level automation,Complacency,"High-level automation in Centaurian systems leads to complacency because human operators may rely too heavily on AI-driven processes, reducing their active engagement and oversight.","""As hybrid systems increasingly share decision-making authority between human and AI components, especially in domains such as healthcare, defense, or finance, it becomes essential to ensure traceable paths of action and define clear loci of control."" (p. 2)","This excerpt highlights the risk of complacency as decision-making authority is shared, potentially leading to reduced human oversight.",y
fhumd-7-1579166,human-computer interaction,Lack of explainability,Loss of situation awareness,"Lack of explainability in AI systems can result in a loss of situation awareness for human operators, as they may not fully understand AI-driven decisions.","""By preserving observable communication layers and explicit transitions, our architectural model supports auditability and human oversight even in deeply integrated Centaurian configurations."" (p. 2)","The need for auditability and oversight suggests that without explainability, human operators may lose awareness of AI actions.",y
fhumd-7-1579166,Human-Computer Interaction,Over-reliance on AI,Skill decay,Over-reliance on AI in Centaurian systems can lead to skill decay as human operators may not practice or maintain their decision-making skills.,"""In Centaurian systems, integration blurs the boundaries between human and artificial components, forming functionally unified entities similar to biological organisms."" (p. 3)",The blurring of boundaries implies that human skills may atrophy as AI takes over more functions.,y
fhumd-7-1579166,AI ethics and accountability,Opaque decision-making,Automation bias,"Opaque decision-making in AI systems can cause automation bias, where human operators favor AI decisions over their own judgment.","""From an ethical and regulatory perspective, the blurring of boundaries between human and artificial agents raises critical issues of accountability and transparency."" (p. 2)","The lack of transparency can lead to automation bias, as operators may defer to AI without understanding its reasoning.",y
frvir-02-686783,human-robot interaction,Conversational abilities in robots,Negative impact on pragmatic quality and mental effort,Conversational abilities in robots lead to increased mental demand and reduced pragmatic quality because the interaction requires more cognitive resources from users.,"""The pragmatic quality and the mental effort indicated a more negative evaluation of the conversational robot than the non-conversational robot."" (p. 12)","This excerpt shows a causal link because it directly compares the impact of conversational abilities on user evaluations, indicating increased cognitive load and reduced task efficiency.",y
frvir-02-686783,Human-Computer Interaction,Embodiment complexity in AI systems,Perceived trustworthiness and friendliness,Complex appearance in AI systems leads to increased perceived trustworthiness and reduced perceived friendliness because users interpret complex designs as more competent but less approachable.,"""The complex appearance led up to less perceived risk (a sign of higher trustworthiness). However, the simple appearance was rated friendlier, and more benevolent (in the correct condition)."" (p. 14)","This excerpt demonstrates a causal relationship by contrasting user perceptions based on AI appearance, highlighting the trade-off between trust and approachability.",y
frvir-02-686783,Extended Reality (XR),Embodied AI in XR environments,Influence on user perception and interaction evaluation,Embodied AI in XR environments influences user perception and interaction evaluation because the immersive experience alters how users perceive social intelligence and engagement.,"""Relatively simple and easy to develop XR environments brought insights into the impact of different social intelligence cues and appearances on a robot and recommender evaluation."" (p. 16)",This excerpt indicates a causal link by showing how XR environments can alter user perceptions and evaluations through immersive interactions.,y
