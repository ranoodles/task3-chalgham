Source,AI Features,Performance Degradation Types,Causal Links,Excerpt,Justification,Validation
2309.10852v2,AI Uncertainty Quantification (UQ),Over-reliance on AI,"AI UQ may lead to over-reliance on AI predictions because it provides additional probabilistic information, which can cause users to trust AI outputs more than necessary.","""AI Uncertainty Quantification (UQ) has the potential to improve human decision-making beyond AI predictions alone by providing additional probabilistic information to users.""","Providing additional probabilistic information can lead users to rely more heavily on AI predictions, potentially reducing their critical assessment of AI outputs. This over-reliance can degrade human performance by diminishing the user's independent decision-making capabilities.",y
2309.10852v2,AI Prediction Accuracy,Reduced human decision-making accuracy,AI predictions alone may reduce human decision-making accuracy because users might not critically evaluate the AI's output.,"""Recent work on human-AI interaction guidelines focuses on explainability and interpretability... However, a quantitative synthesis of studies found that explanations may not generally improve decision accuracy beyond AI prediction alone.""","When users rely solely on AI predictions without additional context or understanding, they may not question or verify the AI's output, leading to potential errors in decision-making. This reliance on AI predictions can degrade human performance by reducing the accuracy of their decisions.",y
2309.10852v2,AI UQ Visualization,Confusion and misinterpretation,"Complex or poorly designed UQ visualizations can lead to confusion and misinterpretation, causing errors in decision-making.","""In the second experiment, we did not find meaningful differences between different representations of UQ... adding more information (here, the UQ distribution), even though task-relevant, did not improve decision-making.""","Providing complex visualizations of UQ information can overwhelm users, leading to confusion and misinterpretation. This can degrade human performance by causing errors in decision-making due to misunderstanding the AI's uncertainty information.",y
26459823,,No relevant evidence found,,,,
41746_2025_Article_1559,High-level automation,Complacency,"High-level automation in AI systems can lead to complacency among healthcare providers because they may overly rely on AI for decision-making, reducing their vigilance and critical thinking.","""It is therefore critical that these AI tools are developed in a manner that avoids automation bias, complacency bias and deskilling.""","This excerpt highlights the risk of complacency bias, which occurs when users overly rely on automated systems, leading to reduced vigilance and critical thinking. The mention of avoiding complacency bias implies a causal link between high-level automation and user complacency.",y
41746_2025_Article_1559,Lack of real-world evaluation frameworks,Skill decay,"Lack of real-world evaluation frameworks for AI in healthcare can lead to skill decay because healthcare providers may not be adequately trained to interact with AI systems in complex, real-world settings.","""The value of medical AI lies in its interaction with human users, yet the lack of evaluation frameworks tailored to real-world environments remains a significant hurdle.""","This excerpt suggests that without proper evaluation frameworks, healthcare providers may not develop or maintain the necessary skills to effectively interact with AI systems, leading to skill decay.",y
41746_2025_Article_1559,Opaque decision-making,Automation bias,"Opaque decision-making in AI systems can lead to automation bias because users may trust AI outputs without understanding the underlying reasoning, leading to errors in judgment.","""It is therefore critical that these AI tools are developed in a manner that avoids automation bias, complacency bias and deskilling.""","The mention of automation bias indicates that opaque decision-making can cause users to trust AI outputs blindly, leading to errors in judgment. This suggests a causal link between the lack of transparency in AI systems and automation bias.",y
41746_2025_Article_1559,Complex AI interactions,Stress and increased workload,Complex interactions with AI systems can increase stress and workload for healthcare providers because they may struggle to adapt to new workflows and decision-making processes.,"""The authors identified a high image-quality related rejection rate of images that human readers could screen, with resultant increased staff workload, elevated stress, and longer patient waiting times.""","This excerpt shows that complex AI interactions, such as image-quality rejections, can lead to increased workload and stress, indicating a causal relationship between the complexity of AI systems and human performance degradation.",y
730709,Agency transference,Skill decay,"Agency transference leads to skill decay as individuals rely on AI to perform tasks, reducing the need to practice and maintain those skills.","""As new technologies emerge and mature, they often take over certain tasks or skills that were previously performed by humans. Off-loading tasks or skills to technology can mean that those skills atrophy over time, a phenomenon known as de-skilling (Wood 1987)."" (Excerpt 5)","This excerpt illustrates how the transfer of agency to AI systems can result in the atrophy of human skills, as tasks once performed by humans are now automated.",y
730709,Parametric reductionism,Dehumanization,"Parametric reductionism leads to dehumanization as AI reduces human complexities to data metrics, failing to capture nuanced human preferences.","""AI functions through parameterization and categorization, reducing the complexities of human beings into a set of quantifiable metrics, classifications, and risk scores to sort, assess, and predict behavior."" (Excerpt 5)","This excerpt shows that AI's reductionist approach can result in the objectification of individuals, which is a form of dehumanization.",y
730709,Regulated expression,Reduced self-expression,Regulated expression leads to reduced self-expression as AI systems require users to simplify their language and interactions.,"""The inherent limitations of current AI systems to convert spoken human language into machine-readable streams of data has caused an unwanted increase in so-called failed 'intent-matching' by the AI system... which makes users switch to more simplified language expressions to interact with these systems."" (Excerpt 7)","This excerpt demonstrates how AI's limitations in language processing lead users to alter their communication, reducing their ability to express themselves fully.",y
730709,Anthropomorphism,Automation bias,"Anthropomorphism leads to automation bias as users attribute human-like qualities to AI, affecting their trust and decision-making.","""Yet, research suggests that even when consumers know that they are interacting with an object or machine, not with a human, they still treat an anthropomorphized object as if it was human—as manifested in consumer behaviors, evaluations, and beliefs."" (Excerpt 1)","This excerpt indicates that anthropomorphism can cause users to trust AI systems as they would humans, potentially leading to biased decision-making.",y
730709,Misaligned objective functions,Misguided decisions,Misaligned objective functions lead to misguided decisions as AI systems prioritize outcomes differently from user preferences.,"""Beyond misalignment in outcome preference, discrepancies can also arise in how outcomes are optimized... In practical terms, this could mean that algorithms often prioritize avoiding large errors when making predictions, while users may prefer them to pursue near-perfect predictions even at the risk of large errors."" (Excerpt 2)","This excerpt highlights how AI's optimization strategies can diverge from human preferences, leading to decisions that do not align with user goals.",y
730709,Privacy paradox,Privacy vulnerability,"Privacy paradox leads to privacy vulnerability as users disclose personal information despite concerns, due to immediate perceived benefits.","""Such self-disclosure has the potential to be harmful... while consumers often express worry over the privacy of the information they provide online, they are also often willing to openly share their most intimate thoughts and feelings in social media posts, responses to online surveys, and chatbots when they feel they are receiving something of value in return."" (Excerpt 2)","This excerpt shows that the privacy paradox results in users sharing sensitive information, increasing their vulnerability despite privacy concerns.",y
Article-4-JDAH-41,Lack of explainability,Skill decay,"""Lack of explainability in AI systems can lead to skill decay as users become less engaged in understanding the decision-making process.""","""There is a clear trade-off between the performance of a machine learning model and its ability to produce explainable and interpretable predictions. Black-box models, which include deep learning and ensembles, are often used in AI, but they are not easily interpretable [12]. The lack of transparency and interpretability of black-box models can lead to serious mistakes, and even dangerous decisions.""","The excerpt discusses the lack of interpretability in AI models and how it can lead to mistakes, implying that users may not engage deeply with the system, leading to skill decay. The lack of transparency reduces the user's ability to understand and interact with the system effectively.",y
Article-4-JDAH-41,Opaque decision-making,Automation bias,"""Opaque decision-making in AI systems can cause automation bias, where users overly trust AI outputs without critical evaluation.""","""The inability to effectively monitor and regulate AI systems has already strained relationships between different industries and regulatory bodies [11]. There is a clear trade-off between the performance of a machine learning model and its ability to produce explainable and interpretable predictions.""","The excerpt highlights the difficulty in monitoring AI systems due to their opaque nature, which can lead to users overly trusting AI outputs, a hallmark of automation bias. This lack of transparency prevents users from critically evaluating AI decisions.",y
Article-4-JDAH-41,Lack of transparency,Complacency,"""Lack of transparency in AI systems can lead to complacency as users may rely on AI decisions without sufficient oversight.""","""The black box problem in AI refers to the difficulty in understanding how AI systems and machine learning models process data and generate predictions or decisions.""","The excerpt describes the black box problem, which can lead to users becoming complacent, as they may not feel the need to understand or oversee the AI's decision-making process due to its opaque nature.",y
Human-AI interaction research agenda_ A user-centered perspective - ScienceDirect,Lack of explainability,Low trust and acceptance,"AI algorithms operate as black boxes, making it difficult for humans to understand the reasoning behind their decisions, leading to low trust and acceptance.","""The low trust in and acceptance of AI among humans nowadays, however, indicates that we are still far from achieving human-AI symbiosis. A fundamental obstacle lies in the fact that many AI algorithms operate as black boxes, making it difficult for humans to understand the reasoning behind their decisions.""","Understanding the decision-making process of AI is crucial for trust. When AI operates as a black box, users cannot comprehend its decisions, leading to distrust and reluctance to accept AI systems.",y
Human-AI interaction research agenda_ A user-centered perspective - ScienceDirect,High-level automation,Job displacement and loss of autonomy,AI's superior productivity and efficiency in tasks lead to job displacement and loss of autonomy for human workers.,"""There is a growing concern about human-AI competition in general, especially with regards to job opportunities. Due to its superior productivity, accuracy, availability, cost-efficiency, and learnability, AI is increasingly replacing humans in relatively simple customer service tasks, and it is also extensively used to supplement the work of professionals such as medical practitioners, lawyers, software engineers, and financial advisors.""","AI's ability to perform tasks more efficiently than humans can lead to job displacement, as AI systems replace human roles, reducing human autonomy in decision-making.",y
Human-AI interaction research agenda_ A user-centered perspective - ScienceDirect,Anthropomorphic design,Uncanny valley effect,"As AI systems become more human-like, subtle imperfections can lead to discomfort and reduced trust from users.","""The uncanny valley hypothesis includes three stages demonstrating different patterns of change in users’ emotional response. First, as robots or virtual characters become more human-like in appearance, users’ affinity or emotional connection will increase correspondingly. Second, when the resemblance reaches a certain level but there still exist subtle imperfections, such anomalies can be detected by users and evoke a feeling of eeriness or discomfort, known as the 'valley'.""","When AI systems are designed to closely resemble humans but have imperfections, it can cause discomfort and reduce trust, impacting user engagement negatively.",y
Human-AI interaction research agenda_ A user-centered perspective - ScienceDirect,Opaque decision-making,Communication breakdowns and user frustration,Opaque decision-making in AI systems leads to communication breakdowns and frustrating user experiences.,"""Communication breakdowns and frustrating user experiences are prevalent in human-AI interfaces. The lack of ethical guidelines or ethical assessment has led to concerns about job displacement, loss of autonomy and control, and even adversarial attacks.""","When users cannot understand AI decisions, it leads to communication issues and frustration, as users cannot effectively interact with or control the AI system.",y
TowardsContext-AwareModelingofSituationAwarenessinConditionallyAutomatedDriving,Conditionally automated driving,Delayed and ineffective driver response,"Conditionally automated driving leads to delayed and ineffective driver response because drivers are distracted by non-driving related tasks (NDRTs), which substantially harms driving safety.","""Research consistently demonstrates that delayed and ineffective driver response during takeovers, especially when distracted by non-driving related tasks (NDRTs), substantially harm driving safety.""","This excerpt shows a causal link between conditionally automated driving and human performance degradation by highlighting how distractions from NDRTs lead to delayed responses, impacting safety.",y
TowardsContext-AwareModelingofSituationAwarenessinConditionallyAutomatedDriving,Automation errors,Decreased trust and increased alertness,"Automation errors lead to decreased trust and increased alertness because drivers become more vigilant and attentive to their surroundings, enhancing situation awareness.","""On the other hand, automation errors (i.e., false alarms and missed TORs) further elevated SA by increasing drivers’ distrust and alertness.""","This excerpt demonstrates a causal link between automation errors and human performance degradation by showing how errors decrease trust, prompting drivers to become more alert and aware.",y
TowardsContext-AwareModelingofSituationAwarenessinConditionallyAutomatedDriving,High-level automation,Mode confusion and automation complacency,"High-level automation leads to mode confusion and automation complacency because drivers struggle to understand operational modes and over-rely on automation, reducing vigilance.","""Mode confusion: Drivers may struggle to understand the current operational mode (manual vs. automated) and their respective roles and responsibilities. Automation complacency: Overreliance on automation can lead to decreased vigilance and reduced monitoring of the driving environment, resulting in impaired SA and increased accident risk.""","This excerpt shows a causal link by explaining how high-level automation causes confusion and complacency, leading to reduced vigilance and impaired situation awareness.",y
TowardsContext-AwareModelingofSituationAwarenessinConditionallyAutomatedDriving,Lack of system transparency,Uncertainty and distrust,"Lack of system transparency leads to uncertainty and distrust because drivers do not understand how the automated system works and its limitations, negatively affecting risk perception and situation awareness.","""System transparency: Understanding how the automated system works and its limitations is crucial for accurate risk assessment. A lack of transparency can lead to uncertainty and distrust, negatively affecting risk perception and SA.""","This excerpt establishes a causal link by indicating how the absence of transparency in the system results in uncertainty and distrust, impacting risk perception and situation awareness.",y
TowardsContext-AwareModelingofSituationAwarenessinConditionallyAutomatedDriving,Automation errors,Enhanced situation awareness,Automation errors enhance situation awareness because experiences with system failures increase perceived risk and driver alertness.,"""Automation error: Drivers’ trust in the automated system’s capabilities significantly impacts their risk perception. If the system is perceived as reliable, drivers may be less vigilant and have lower SA. Conversely, experiences with system failures or unexpected behavior can increase perceived risk and enhance SA.""","This excerpt shows a causal link by describing how system failures lead to increased perceived risk, which in turn enhances situation awareness.",y
amiajnl-2011-000089,Adaptive automation,Complacency: A state where users overly rely on automation and reduce their vigilance.,"Adaptive automation can lead to complacency because users may trust the system to manage tasks without their intervention, reducing their active monitoring and engagement.","""Parasuraman R, Molloy R, Singh IL. Performance consequences of automation-induced ‘complacency’. Int J Aviat Psychol 1993;3:1e23.""","This excerpt discusses the performance consequences of automation-induced complacency, indicating a direct link between adaptive automation and reduced user vigilance.",y
amiajnl-2011-000089,Imperfect alerting,Automation bias: A tendency to favor automated suggestions over human judgment.,"Imperfect alerting can cause automation bias as users may overly trust the system's alerts, even when they are incorrect, leading to poor decision-making.","""Wickens C, Colcombe A. Dual-task performance consequences of imperfect alerting associated with a cockpit display of trafﬁc information. Hum Factors 2007;49:5839e50.""","The excerpt highlights the dual-task performance consequences of imperfect alerting, suggesting that users may develop a bias towards automated alerts, affecting their decision-making.",y
amiajnl-2011-000089,Decision support systems,Automation bias: A tendency to favor automated suggestions over human judgment.,"Decision support systems can lead to automation bias as users may rely on the system's recommendations, potentially disregarding their judgment.","""Madhavan P, Wiegmann DA. Effects of information source, pedigree, and reliability on operator interaction with decision support systems. Hum Factors 2007;49:5773e85.""","This excerpt discusses how decision support systems affect operator interaction, indicating that reliance on these systems can lead to automation bias.",y
amiajnl-2011-000089,Computer-assisted detection,Performance evaluation: A measure of how well users perform tasks with the aid of automation.,"Computer-assisted detection can affect performance evaluation as it may alter the way tasks are completed, impacting both experienced and inexperienced users differently.","""Marten K, Seyfarth T, Auer F, et al. Computer-assisted detection of pulmonary nodules: performance evaluation of an expert knowledge-based detection system in consensus reading with experienced and inexperienced chest radiologists. Eur Radiol 2004;14:1930e8.""","This excerpt evaluates the performance of users with computer-assisted detection, showing how automation can impact task performance differently based on user experience.",y
amiajnl-2011-000089,Automated decision aids,Complacency: A state where users overly rely on automation and reduce their vigilance.,"Automated decision aids can lead to complacency because users may become less vigilant, assuming the system will handle decision-making accurately.","""Bahner JE, Huper AD, Manzey D. Misuse of automated decision aids: Complacency, automation bias and the impact of training experience. Int J Hum Comput Stud 2008;66:688e99.""","This excerpt discusses the misuse of automated decision aids, linking their use to increased complacency and reduced vigilance in users.",y
fhumd-7-1579166,High-level automation,Skill decay,"High-level automation in AI systems, such as LAMs, can lead to skill decay because human operators rely on AI for decision-making, reducing their engagement in active problem-solving.","""The LAM framework demonstrated its ability to refine its operations based on continuous feedback from human interactions, promoting an adaptive and responsive HCI system.""","The excerpt suggests that the LAM framework takes over decision-making processes, which may reduce the need for human operators to engage in these tasks, potentially leading to skill decay.",y
fhumd-7-1579166,Lack of explainability,Frustration,"Lack of explainability in AI systems can lead to frustration among users because they cannot understand or predict AI decisions, leading to errors and negative consequences.","""Instead, they found significantly higher frustration levels, identifying five distinct errors resulting from violations of human-AI interaction guidelines, leading to various (negative) consequences for the participants.""",The excerpt directly links the lack of explainability in AI systems to increased frustration levels due to errors and negative consequences experienced by users.,y
fhumd-7-1579166,Opaque decision-making,Complacency,"Opaque decision-making in AI systems can lead to complacency because users may over-rely on AI outputs without understanding the underlying processes, reducing their vigilance.","""This ensures adaptability while maintaining traceability—a key requirement for Centaurian systems in high-stakes domains.""","The emphasis on traceability suggests that without it, users may not question AI decisions, leading to complacency.",y
fhumd-7-1579166,Over-trust prompts,Loss of situation awareness,"Over-trust in AI prompts can lead to a loss of situation awareness because users may defer to AI suggestions without critically evaluating the context, diminishing their situational engagement.","""The system either accepts the LLM’s output with a confidence threshold (> 80%) or defaults to the symbolic rule, logging the conflict for later refinement.""","The reliance on AI outputs with a confidence threshold indicates that users may not engage with the situation critically, leading to a loss of awareness.",y
frvir-02-686783,Conversational ability in robots,Negative interaction evaluation,Conversational ability in robots can lead to negative evaluations of interaction due to increased mental demand and decreased pragmatic quality.,"""The pragmatic quality and the mental effort indicated a more negative evaluation of the conversational robot than the non-conversational robot.""","The excerpt indicates that the conversational ability of the robot increased mental demand and decreased pragmatic quality, leading to a negative evaluation of the interaction. This suggests that the added complexity of conversation may overwhelm users, impacting their performance negatively.",y
frvir-02-686783,Complex appearance of AI systems,Perceived risk and trust issues,"Complex appearance of AI systems can lead to less perceived risk and higher trustworthiness, but may also lead to overestimation of AI capabilities.","""The complex appearance led up to less perceived risk (a sign of higher trustworthiness).""","The complex appearance of the AI system reduced perceived risk, which could lead to over-trust and potential complacency, as users might overestimate the AI's capabilities and reliability.",y
frvir-02-686783,Embodiment of AI systems,Anthropomorphization and misperception,"Embodiment of AI systems can lead to anthropomorphization, causing users to misperceive AI capabilities and intentions.","""The media equation postulates that the sheer interaction itself already contributes signiﬁcantly to the perception and, more speciﬁcally, an anthropomorphization of technical systems by users.""","The embodiment of AI systems leads users to attribute human-like qualities to them, which can result in misperceptions about the AI's capabilities and intentions, affecting decision-making and interaction outcomes.",y
frvir-02-686783,Social intelligence cues in robots,Gender-based perception differences,Social intelligence cues in robots can lead to gender-based differences in perception and interaction evaluation.,"""Women rated the conversational robot as more positive (signiﬁcant) and attractive (by trend) than the non-conversational robot, while men did not show any differences.""","The presence of social intelligence cues in robots resulted in different evaluations based on gender, suggesting that these cues can influence how users perceive and interact with AI, potentially leading to biased or inconsistent performance outcomes.",y
