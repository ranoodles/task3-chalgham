Source,AI Features,Performance Degradation Types,Causal Links,Excerpt,Justification,Validation
2309.10852v2,Overconfidence in AI predictions,Over-reliance on AI predictions,"Overconfidence in AI predictions can lead to over-reliance on AI predictions by human users, as they might trust AI outputs more than their judgment.","""However, prediction probabilities are prone to overconfidence in some AI models. There is a lack of discussion on the calibration of uncertainty estimates in the existing literature."" (p. 3)","This excerpt suggests that AI models can be overconfident in their predictions, which might lead users to rely too heavily on AI outputs without questioning them.",y
2309.10852v2,AI Uncertainty Quantification (UQ),Complexity in decision-making,"AI UQ can introduce complexity in decision-making because users need to interpret probabilistic information, which may not always be straightforward.","""However, increasing information, even when it is task-relevant, is not always beneficial to human decision-making performance."" (p. 2)","This excerpt indicates that while UQ provides more information, it can complicate decision-making, potentially leading to decreased performance.",y
2309.10852v2,AI Uncertainty Quantification (UQ),No significant improvement in decision-making accuracy,"AI UQ does not always lead to improved decision-making accuracy, as users might struggle to interpret and apply the uncertainty information effectively.","""Despite previous general findings that uncertainty information is useful for decision-making, there is limited behavioral research assessing the benefits of AI UQ, particularly for human decision-making accuracy."" (p. 2)","This excerpt highlights that the expected benefits of AI UQ on decision-making accuracy are not consistently observed, suggesting potential issues in user interpretation or application of UQ.",y
41746_2025_Article_1559,High-level automation,Complacency bias,"High-level automation in AI decision-support tools can lead to complacency bias because it reduces the need for active human decision-making, which may result in over-reliance on AI outputs.","""It is therefore critical that these AI tools are developed in a manner that avoids automation bias, complacency bias and deskilling."" (p. 4)","This excerpt indicates a direct concern that high-level automation in AI tools can lead to complacency, as users might overly rely on AI, reducing their active engagement and decision-making.",y
41746_2025_Article_1559,AI-enabled decision-support tools,Deskilling,"AI-enabled decision-support tools can lead to deskilling because they take over tasks that require specific skills, leading to skill decay over time.","""It is therefore critical that these AI tools are developed in a manner that avoids automation bias, complacency bias and deskilling."" (p. 4)","This statement highlights the risk of deskilling, as AI tools performing tasks traditionally done by humans can lead to a reduction in skill levels due to lack of practice.",y
41746_2025_Article_1559,Poorly designed systems,User satisfaction and efficiency reduction,"Poorly designed AI systems reduce user satisfaction and efficiency because they create frustration and inefficiencies in workflow, impacting overall performance.","""For example, poorly designed systems reduce both user satisfaction and efficiency, as seen with electronic health record (EHR) systems."" (p. 2)","This excerpt shows that design flaws in AI systems can lead to decreased satisfaction and efficiency, indicating a direct impact on human performance.",y
41746_2025_Article_1559,AI-based automatic detection of psychophysiological states,Stress,"AI-based automatic detection systems can increase stress because they may misinterpret user states, leading to inappropriate responses or actions.","""AI-enabled decision-support tools act as human-AI interfaces, and therefore interfaces that influence cognitive and emotional states of medical decision-makers."" (p. 4)","This indicates that AI systems can impact stress levels by influencing emotional states, potentially through misinterpretation or inappropriate feedback.",y
730709,Recommendation algorithms,Loss of serendipity,"Recommendation algorithms limit serendipity by reinforcing past behaviors, which constrains exploration and change, leading to reduced human agency.","""However, recommendation algorithms often limit our experience of serendipity. They do so because most of these systems are set up to feed content based on our past behaviors... Consequently, they reinforce those behaviors and create inertia that limits exploration and change..."" (p. 3)","This excerpt shows that recommendation algorithms reinforce existing behaviors, which limits new experiences and exploration, thereby reducing human agency.",y
730709,Generative AI models,De-skilling,"Relying on generative AI models for tasks like creative idea generation can lead to de-skilling as humans forego practicing these skills, causing them to weaken over time.","""The emergence of ChatGPT and other forms of generative AI have revived the concern that AI could contribute to a new wave of de-skilling among knowledge workers... If workers begin to rely on AI to perform parts of their jobs, such as writing or brainstorming, might they be foregoing opportunities to practice and strengthen those skills?"" (p. 4)","This excerpt indicates that reliance on AI for creative tasks can lead to skill atrophy, as humans do not practice and strengthen these skills, resulting in de-skilling.",y
730709,Voice-activated AI assistants,Constrained interaction,"Voice-activated AI assistants cause users to simplify their language, which can lead to reduced linguistic diversity and altered communication styles.","""Qualitative research with Amazon Alexa users revealed that they often engage in a more simplified form of communication compared to a human interlocutor... conversational AI systems implicitly disincentivize linguistic diversity by requiring users to employ more simplistic, reduced forms of language."" (p. 7)","This excerpt shows that users simplify their language when interacting with AI, which can lead to reduced linguistic diversity and altered communication styles, impacting human communication skills.",y
730709,AI systems in social media,Constrained self-expression,"AI systems in social media can constrain self-expression by evoking certain aspects of identity while suppressing others, leading to inauthentic self-expression.","""For instance, people perceive that AI-systems are meant to amplify some aspects of their identity, while filtering out or suppressing other, more marginalized, parts... preliminary experimental research suggests that gender-stereotypical recommendations decrease women's self-reported masculinity, leadership ability, and self-confidence."" (p. 8)","This excerpt demonstrates that AI systems can constrain self-expression by emphasizing certain identity aspects over others, leading to inauthentic self-expression and impacting self-perception.",y
730709,AI-driven algorithms,Reduced autonomy,"AI-driven algorithms that reinforce past choices can curtail perceived autonomy, which is detrimental to consumer well-being.","""First, when AI-driven algorithms only reinforce past choices, users’ perceived autonomy is curtailed... This psychological outcome... has important public policy implications since the sense of a lack of autonomy has been found to be detrimental to consumer well-being."" (p. 10)","This excerpt highlights that AI algorithms reinforcing past choices reduce perceived autonomy, which negatively affects consumer well-being by limiting their sense of control.",y
Article-4-JDAH-41,Lack of transparency and interpretability,Skill decay,"""Lack of transparency and interpretability in black-box models can lead to serious mistakes, and even dangerous decisions.""","""The lack of transparency and interpretability of black-box models can lead to serious mistakes, and even dangerous decisions."" p. 33","The excerpt indicates that the lack of transparency in AI models can result in errors, implying that users may not develop or maintain necessary skills to effectively oversee or correct these models.",y
Article-4-JDAH-41,Black-box AI models,Automation bias,"""The inability to effectively monitor and regulate AI systems has already strained relationships between different industries and regulatory bodies.""","""The inability to effectively monitor and regulate AI systems has already strained relationships between different industries and regulatory bodies."" p. 33","This statement suggests that users may overly rely on AI outputs without sufficient oversight, leading to automation bias where human judgment is overshadowed by AI decisions.",y
Article-4-JDAH-41,Opaque decision-making,Complacency,"""XAI techniques enable users to gain a deeper understanding of how AI models arrive at their decisions, thus increasing trust in the system.""","""XAI techniques enable users to gain a deeper understanding of how AI models arrive at their decisions, thus increasing trust in the system."" p. 34","The excerpt implies that without XAI, users might not understand AI decisions, potentially leading to complacency as they might not question or verify AI outputs.",y
Human-AI interaction research agenda_ A user-centered perspective - ScienceDirect,High-level automation,Skill decay,"High-level automation in autonomous vehicles and service robots leads to skill decay because humans rely on AI for tasks they previously performed, reducing their engagement and practice.","""Unlike industrial robots that are programmed to perform simple repetitive tasks, e.g., welding and assembly, service robots (e.g., Plato, NAO, and Pepper) are embodied robots designed to interact with and provide personalized services to humans with a high degree of autonomy."" (p. 6)","This excerpt indicates that service robots take over tasks that humans used to perform, which can lead to a reduction in human skill levels as they become less engaged in these tasks.",y
Human-AI interaction research agenda_ A user-centered perspective - ScienceDirect,Lack of explainability,Trust issues,"Lack of explainability in AI systems leads to trust issues because users cannot understand the reasoning behind AI decisions, causing reluctance to rely on AI outputs.","""The low trust in and acceptance of AI among humans nowadays, however, indicates that we are still far from achieving human-AI symbiosis. A fundamental obstacle lies in the fact that many AI algorithms operate as black boxes, making it difficult for humans to understand the reasoning behind their decisions."" (p. 11)","This excerpt shows that the opacity of AI decision-making processes undermines user trust, as users are unable to comprehend how AI arrives at its conclusions.",y
Human-AI interaction research agenda_ A user-centered perspective - ScienceDirect,Over-trust prompts,Automation bias,Over-trust prompts in AI systems lead to automation bias because users may overly rely on AI decisions without critical evaluation.,"""Humans should make better use of their invaluable expertise in creative approaches, emotional intelligence, and complex problem-solving, while AI can be leveraged for its incredible computational capabilities for pattern recognition, reasoning, prediction, and decision making."" (p. 9)","This excerpt suggests that while AI can be beneficial, there is a risk that users may over-rely on AI, leading to automation bias where human judgment is not adequately applied.",y
Human-AI interaction research agenda_ A user-centered perspective - ScienceDirect,Anthropomorphic design,Uncanny valley effect,Anthropomorphic design in AI systems can lead to the uncanny valley effect because users may feel discomfort when AI appears almost human-like but not quite perfect.,"""The uncanny valley hypothesis includes three stages demonstrating different patterns of change in users’ emotional response. First, as robots or virtual characters become more human-like in appearance, users’ affinity or emotional connection will increase correspondingly. Second, when the resemblance reaches a certain level but there still exist subtle imperfections, such anomalies can be detected by users and evoke a feeling of eeriness or discomfort, known as the 'valley'."" (p. 14)","This excerpt explains how AI systems designed to closely mimic human appearance can cause discomfort if they do not achieve perfect human likeness, leading to negative user reactions.",y
Human-AI interaction research agenda_ A user-centered perspective - ScienceDirect,Opaque decision-making,Complacency,"Opaque decision-making in AI systems leads to complacency because users may not question AI outputs, assuming they are correct without verification.","""A novel concept, known as human-centered AI, has been put forth to address the above challenges. It emphasizes that the design, development and deployment of AI technologies and systems should focus on the needs, values, and well-being of humans, with an aim to empower individuals and promote positive outcomes for society."" (p. 11)",This excerpt highlights the need for transparency in AI systems to prevent users from becoming complacent and failing to critically assess AI outputs.,y
TowardsContext-AwareModelingofSituationAwarenessinConditionallyAutomatedDriving,Conditionally automated driving (SAE Level 3),Decreased vigilance and reduced monitoring,"Conditionally automated driving can lead to decreased vigilance and reduced monitoring because drivers over-rely on automation, resulting in impaired situation awareness and increased accident risk.","""Automation complacency: Overreliance on automation can lead to decreased vigilance and reduced monitoring of the driving environment, resulting in impaired SA and increased accident risk (Thill, Hemeren, & Nilsson, 2014)."" (p. 5)","This excerpt shows a causal link as it directly connects the feature of conditionally automated driving with decreased vigilance and monitoring, explaining that overreliance on automation impairs situation awareness.",y
TowardsContext-AwareModelingofSituationAwarenessinConditionallyAutomatedDriving,Mode confusion,Delayed responses and inappropriate actions,Mode confusion in conditionally automated driving leads to delayed responses and inappropriate actions because drivers struggle to understand the current operational mode and their responsibilities.,"""Mode confusion: Drivers may struggle to understand the current operational mode (manual vs. automated) and their respective roles and responsibilities. This uncertainty can lead to inappropriate actions and delayed responses in critical situations (Merat & Jamson, 2009)."" (p. 5)","This excerpt provides a causal link by explaining how mode confusion affects driver performance, leading to delayed responses and inappropriate actions.",y
TowardsContext-AwareModelingofSituationAwarenessinConditionallyAutomatedDriving,Automation errors,Enhanced situation awareness,"Automation errors can enhance situation awareness because they increase drivers' distrust and alertness, leading to improved monitoring of the driving environment.","""Automation errors (i.e., false alarms and missed TORs) further elevated SA by increasing drivers’ distrust and alertness."" (p. 23)","This excerpt establishes a causal link by showing how automation errors affect drivers' situation awareness, increasing their alertness and monitoring.",y
TowardsContext-AwareModelingofSituationAwarenessinConditionallyAutomatedDriving,High-level automation,Complacency,"High-level automation leads to complacency because drivers engage in non-driving related tasks, reducing their situation awareness and takeover performance.","""However, driver engagement in NDRTs can lead to degradation of SA, leading to failures in takeovers (Endsley & Kiris, 1995; Körber, Baseler, & Bengler, 2018)."" (p. 3)","This excerpt demonstrates a causal link by explaining how high-level automation encourages engagement in non-driving tasks, degrading situation awareness and takeover performance.",y
amiajnl-2011-000089,Automation bias,Errors of commission and omission,Automation bias leads to errors of commission and omission because users tend to over-accept computer output as a heuristic replacement of vigilant information seeking and processing.,"""AB manifests in errors of commission (following incorrect advice) and omission (failing to act because of not being prompted to do so) when using CDSS."" (p. 1)",This excerpt shows a causal link because it explicitly states that automation bias results in specific errors due to over-reliance on computer output.,y
amiajnl-2011-000089,High-level automation,Complacency,High-level automation leads to complacency because users allocate attention to manual tasks over monitoring automation output.,"""Complacency appears to occur as an attention allocation strategy in multitasking where manual tasks are attended to over monitoring the veracity of the output of automation."" (p. 1)","This excerpt shows a causal link because it describes how high-level automation affects attention allocation, leading to insufficient monitoring.",y
amiajnl-2011-000089,Lack of training,Over-reliance on automation,Lack of training increases over-reliance on automation because users are less likely to recognize incorrect advice.,"""Training may increase the likelihood of recognizing DSS error and thus reduce AB (particularly commission errors)."" (p. 5)",This excerpt shows a causal link because it suggests that training can mitigate over-reliance by helping users identify errors.,y
amiajnl-2011-000089,Trust in automation,Automation bias,Trust in automation increases automation bias because it leads to over-reliance on DSS output even when it is incorrect.,"""Trust is possibly the strongest driving factor in over-reliance, when trust is incorrectly calibrated against system reliability."" (p. 4)","This excerpt shows a causal link because it identifies trust as a key factor that influences the degree of reliance on automation, leading to bias.",y
amiajnl-2011-000089,Display prominence,Automation bias,Display prominence increases automation bias because prominent incorrect advice is more likely to be followed.,"""Berner et al found that display prominence increased AB, affecting the likelihood of a decision being changed after advice."" (p. 6)",This excerpt shows a causal link because it connects the design feature of display prominence to increased likelihood of following incorrect advice.,y
fhumd-7-1579166,Agentic AI systems,Complacency,"Agentic AI systems lead to complacency because they handle complex, multi-step problems autonomously, reducing the need for human intervention.","""Agentic AI systems engage in sophisticated reasoning and iterative planning to solve complex, multi-step problems autonomously."" (p. 4)","This excerpt shows that agentic AI systems can perform tasks independently, which may cause human operators to become complacent, as they rely on the AI to manage tasks without their input.",y
fhumd-7-1579166,Integration with LLM and human operators,Over-reliance on AI,"Integration with LLM and human operators can lead to over-reliance on AI because human judgment and AI recommendations are seamlessly integrated, reducing human critical thinking.","""However, the interaction between human operators and the LLM exhibits Centaurian characteristics, creating a tightly coupled decision unit where human judgment and AI recommendations are seamlessly integrated."" (p. 10)","This excerpt indicates that the seamless integration of human judgment and AI recommendations may cause humans to overly rely on AI, potentially diminishing their decision-making skills.",y
fhumd-7-1579166,Neuro-symbolic integration,Skill decay,"Neuro-symbolic integration can lead to skill decay because it combines neural network capabilities with symbolic AI, reducing the need for human cognitive processing.","""By combining neural network capabilities for pattern recognition with symbolic AI for rule-based processing, LAM demonstrates how apparently distinct computational approaches can be unified into a coherent whole."" (p. 11)","This excerpt suggests that the integration of neural and symbolic AI reduces the need for human cognitive processing, potentially leading to skill decay as humans rely more on AI for complex tasks.",y
fhumd-7-1579166,Feedback loops in LAM,Automation bias,"Feedback loops in LAM can lead to automation bias because they create a dynamic system where human and artificial components grow together, reinforcing reliance on AI outputs.","""LAM’s feedback loops create a dynamic system where human and artificial components grow together, continuously refining their interaction patterns."" (p. 12)","This excerpt shows that the continuous refinement of interaction patterns between humans and AI in LAM can reinforce reliance on AI outputs, leading to automation bias.",y
frvir-02-686783,Conversational ability of robots,Negative impact on pragmatic quality and increased mental effort,Conversational ability in robots leads to increased mental effort and decreased pragmatic quality because it introduces additional cognitive load and potential distraction during tasks.,"""The pragmatic quality and the mental effort indicated a more negative evaluation of the conversational robot than the non-conversational robot."" (p. 10)","This excerpt shows a causal link as it directly associates the conversational ability of robots with increased mental effort and decreased pragmatic quality, suggesting that the added social interaction can be distracting and cognitively demanding.",y
frvir-02-686783,Appearance complexity of recommender systems,Perceived friendliness and risk perception,Complex appearance in recommender systems leads to lower perceived friendliness and higher perceived risk because users may associate complexity with unpredictability or lack of transparency.,"""The simple appearance was rated friendlier, and more benevolent (in the correct condition)."" (p. 13)","This excerpt indicates that a simpler appearance is perceived as friendlier and less risky, suggesting that complexity can negatively impact user perception by making the system seem less approachable or trustworthy.",y
frvir-02-686783,Embodiment of AI systems,Eliza effect (overestimation of AI capabilities),Embodiment of AI systems can lead to the Eliza effect because users may attribute more intelligence and competence to embodied systems than they possess.,"""The second experiment reveals an Eliza effect of a recommender system."" (p. 17)","The mention of the Eliza effect suggests that the embodiment of AI systems can cause users to overestimate the system's capabilities, leading to potential misjudgments about its competence.",y
