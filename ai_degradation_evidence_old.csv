Source,AI Features,Performance Degradation Types,Causal Links,Excerpt,Justification,Validation
2309.10852v2,AI Uncertainty Quantification (UQ),None explicitly mentioned,"UQ may improve decision-making accuracy, but poorly calibrated UQ can be detrimental.","""Our method for AI UQ uses known class labels to ensure high-quality uncertainty information at the instance-level, as poorly calibrated uncertainty information is likely to be detrimental to decision-making."" (p. 2)","This excerpt suggests that while well-calibrated UQ can improve decision-making, poorly calibrated UQ can degrade performance, implying a potential causal link.",y
2309.10852v2,AI Predictions,Overconfidence,"AI predictions can lead to overconfidence, affecting decision-making accuracy.","""However, prediction probabilities are prone to overconfidence in some AI models. There is a lack of discussion on the calibration of uncertainty estimates in the existing literature."" (p. 3)","This excerpt indicates that overconfidence in AI predictions can lead to decision-making errors, suggesting a causal link between AI predictions and overconfidence.",y
2309.10852v2,AI Predictions,None explicitly mentioned,AI predictions alone do not always improve decision accuracy compared to human decision-making.,"""One less-explored possibility for promoting effective human-AI interaction is AI Uncertainty Quantification (UQ) for predictions. AI UQ is posited to be key for human decision-making (Abdar et al., 2021b; Jalaian et al., 2019)."" (p. 1)","This excerpt implies that AI predictions alone may not enhance decision accuracy, suggesting a need for additional features like UQ.",y
2309.10852v2,AI Uncertainty Quantification (UQ),None explicitly mentioned,High-quality UQ improves decision-making accuracy and confidence calibration.,"""We found that providing high-quality AI UQ meaningfully improves decision-accuracy and confidence calibration over an AI prediction alone."" (p. 1)",This excerpt shows a direct causal link between high-quality UQ and improved decision-making performance.,y
2309.10852v2,AI Predictions,None explicitly mentioned,AI predictions alone may not enhance decision accuracy compared to human decision-making.,"""Despite previous general findings that uncertainty information is useful for decision-making, there is limited behavioral research assessing the benefits of AI UQ, particularly for human decision-making accuracy."" (p. 2)","This excerpt suggests that AI predictions alone may not improve decision accuracy, indicating a potential limitation of AI predictions.",y
41746_2025_Article_1559,Poorly designed AI systems,Reduced user satisfaction and efficiency,"Poorly designed AI systems reduce both user satisfaction and efficiency, as seen with electronic health record (EHR) systems.","""For example, poorly designed systems reduce both user satisfaction and efficiency, as seen with electronic health record (EHR) systems."" (p. 2)","This excerpt highlights that the design of AI systems can directly impact user satisfaction and efficiency, indicating a causal relationship between system design and human performance degradation.",y
41746_2025_Article_1559,AI-enabled decision-support tools,"Automation bias, complacency bias, and deskilling","AI-enabled decision-support tools can lead to automation bias, complacency bias, and deskilling because they influence cognitive and emotional states of medical decision-makers.","""It is therefore critical that these AI tools are developed in a manner that avoids automation bias, complacency bias and deskilling."" (p. 4)","This statement explicitly links AI-enabled decision-support tools to specific types of human performance degradation, emphasizing the need for careful development to avoid these issues.",y
41746_2025_Article_1559,High image-quality requirements in AI systems,"Increased staff workload, elevated stress, longer patient waiting times","High image-quality requirements in AI systems increase staff workload, stress, and patient waiting times because they lead to a high rejection rate of images that human readers could screen.","""The authors identified a high image-quality related rejection rate of images that human readers could screen, with resultant increased staff workload, elevated stress, and longer patient waiting times."" (p. 5)","This excerpt shows a direct causal link between the AI feature of high image-quality requirements and human performance degradation in terms of workload, stress, and efficiency.",y
730709,Recommendation algorithms,Loss of serendipity,"Recommendation algorithms limit serendipity by reinforcing past behaviors, which restricts exploration and change, thus constraining human agency.","""However, recommendation algorithms often limit our experience of serendipity. They do so because most of these systems are set up to feed content based on our past behaviors (André et al. 2018; Wertenbroch et al. 2020). Consequently, they reinforce those behaviors and create inertia that limits exploration and change (Talaifar and Lowery 2023), constraining human agency."" (p. 3)","This excerpt shows that recommendation algorithms reinforce past behaviors, limiting exploration and change, which constrains human agency by reducing exposure to new options.",y
730709,Generative AI models,De-skilling,"Generative AI models contribute to de-skilling by performing tasks that reduce opportunities for humans to practice and strengthen their skills, leading to skill atrophy.","""The emergence of ChatGPT and other forms of generative AI have revived the concern that AI could contribute to a new wave of de-skilling among knowledge workers. For example, generative AI models can now outperform most humans at creative idea generation (Guzik, Byrge, and Gilde 2023; Koivisto and Grassini 2023). Relying on such models can increase the quantity and quality of knowledge workers’ output (Noy and Zhang 2023; Dell’Acqua et al. 2024). If workers begin to rely on AI to perform parts of their jobs, such as writing or brainstorming, might they be foregoing opportunities to practice and strengthen those skills?"" (p. 4)","This excerpt indicates that reliance on generative AI models for tasks like writing or brainstorming may lead to skill atrophy, as humans forego opportunities to practice and strengthen these skills.",y
730709,AI-based social media algorithms,Identity constraints,"AI-based social media algorithms constrain self-expression by evoking certain aspects of identity while suppressing others, leading to a narrowed self-perception.","""Indeed, people seem to understand that algorithms in these environments are tailored to influence their identities in specific, potentially pernicious, ways (Bhandari and Bimo 2022). For instance, people perceive that AI-systems are meant to amplify some aspects of their identity, while filtering out or suppressing other, more marginalized, parts (Simpson and Semaan 2021)."" (p. 8)","This excerpt highlights how AI-based social media algorithms can constrain self-expression by selectively amplifying or suppressing aspects of identity, leading to a narrowed self-perception.",y
730709,Voice-activated AI assistants,Constrained interaction,"Voice-activated AI assistants lead to constrained interaction by requiring users to simplify their language, which reduces linguistic diversity and authentic self-expression.","""Qualitative research with Amazon Alexa users revealed that they often engage in a more simplified form of communication compared to a human interlocutor (Ammari et al. 2019). Recent research by Hildebrand, Hoffman, and Novak (2023) provides large-scale evidence using a corpus of actual user-voice assistant interactions that the default modality of users is a more direct, imperative language style compared to human-to-human communication, using fewer personal pronouns (such as 'I' or 'me'), shorter sentence lengths with fewer words, and an overall less polite language style (not saying 'please' when asking the device to perform a task)."" (p. 7)","This excerpt shows that voice-activated AI assistants constrain interaction by requiring simplified language, which reduces linguistic diversity and authentic self-expression.",y
730709,AI decision-making systems,Misalignment,"Misalignment occurs when AI decision-making systems use reductionist representations that fail to capture complex human preferences, leading to decisions that do not align with user desires.","""Secondly, because the algorithms powering today’s AI systems are designed to rely on a reductionist representation of user interests, any discrepancy between this simplified representation and the complexity of actual human preferences risks a misalignment between AI recommendations and what users truly want or would choose for themselves."" (p. 5)","This excerpt illustrates how AI decision-making systems can misalign with human preferences due to their reliance on reductionist representations, leading to decisions that do not align with user desires.",y
Article-4-JDAH-41,Lack of transparency and interpretability,Skill decay,"""Lack of transparency and interpretability in black-box models can lead to serious mistakes, and even dangerous decisions.""","""The lack of transparency and interpretability of black-box models can lead to serious mistakes, and even dangerous decisions."" (p. 33)","Black-box AI models' lack of transparency and interpretability can cause human operators to rely on AI without understanding, leading to potential errors and skill decay.",y
Article-4-JDAH-41,Black-box AI models,Automation bias,"""These models often rely on intricate algorithms that are not easily understandable to humans, leading to a lack of accountability and trust.""","""When AI models are not understandable, users may overly rely on them, leading to automation bias where users trust AI outputs without critical evaluation."" (p. 33)","The complexity and lack of understanding of black-box AI models can cause users to trust AI outputs without questioning, resulting in automation bias.",y
Article-4-JDAH-41,Opaque decision-making,Complacency,"""XAI techniques enable users to gain a deeper understanding of how AI models arrive at their decisions, thus increasing trust in the system.""","""Without XAI, users may not understand AI decisions, leading to complacency as they may not feel the need to monitor AI outputs critically."" (p. 34)","The lack of explainability in AI systems can lead to user complacency, as they may not engage in active oversight of AI decisions.",y
Human-AI interaction research agenda_ A user-centered perspective - ScienceDirect,High-level automation,Skill decay,High-level automation in autonomous vehicles can lead to skill decay as drivers rely less on their driving skills.,"""For lack of trust, however, public acceptance of and consumer readiness for autonomous vehicles remain low at present (Alawadhi et al., 2020).""","This excerpt suggests that the reliance on autonomous vehicles could lead to a decrease in the need for human driving skills, potentially causing skill decay.",y
Human-AI interaction research agenda_ A user-centered perspective - ScienceDirect,Opaque decision-making,Loss of trust,Opaque decision-making in AI systems leads to a loss of trust because users cannot understand or predict AI behavior.,"""A fundamental obstacle lies in the fact that many AI algorithms operate as black boxes, making it difficult for humans to understand the reasoning behind their decisions.""","This excerpt highlights the issue of AI systems being opaque, which can lead to a loss of trust as users struggle to understand AI decisions.",y
Human-AI interaction research agenda_ A user-centered perspective - ScienceDirect,Anthropomorphic design,Uncanny valley effect,"Anthropomorphic design can lead to the uncanny valley effect, causing discomfort among users.","""With great efforts devoted to addressing the uncanny valley, a phenomenon in which people something that is almost but not fully human-like causes feelings of unease or revulsion in the observer (Mori et al., 2012).""","This excerpt explains how anthropomorphic design can lead to the uncanny valley effect, which negatively impacts user comfort.",y
Human-AI interaction research agenda_ A user-centered perspective - ScienceDirect,Automation in service robots,Dehumanization,Automation in service robots can lead to dehumanization as human roles are replaced by machines.,"""With the wide adoption of service robots, there rise several major ethical concerns, including privacy, dehumanization, social deprivation, and disempowerment (Čaić et al., 2019).""","This excerpt indicates that the use of service robots can lead to dehumanization, as human roles and interactions are replaced by automated systems.",y
Human-AI interaction research agenda_ A user-centered perspective - ScienceDirect,AI in gaming,Challenge-skill imbalance,"AI in gaming can create a challenge-skill imbalance, leading to either frustration or lack of engagement.","""Both unbeatable and incompetent AI players are undesirable, what level of difficulty is appropriate for a game that offers both challenges and entertainment?""","This excerpt discusses how AI in gaming can lead to a challenge-skill imbalance, affecting user engagement and satisfaction.",y
TowardsContext-AwareModelingofSituationAwarenessinConditionallyAutomatedDriving,High-level automation,Mode confusion,"High-level automation leads to mode confusion because drivers struggle to understand the current operational mode and their responsibilities, leading to inappropriate actions.","""Mode confusion: Drivers may struggle to understand the current operational mode (manual vs. automated) and their respective roles and responsibilities. This uncertainty can lead to inappropriate actions and delayed responses in critical situations (Merat & Jamson, 2009)."" (p. 5)","This excerpt shows a causal link between high-level automation and mode confusion, as it describes how drivers' uncertainty about the operational mode leads to inappropriate actions.",y
TowardsContext-AwareModelingofSituationAwarenessinConditionallyAutomatedDriving,High-level automation,Automation complacency,"High-level automation leads to automation complacency because drivers over-rely on automation, decreasing vigilance and monitoring of the driving environment.","""Automation complacency: Overreliance on automation can lead to decreased vigilance and reduced monitoring of the driving environment, resulting in impaired SA and increased accident risk (Thill, Hemeren, & Nilsson, 2014)."" (p. 5)","This excerpt shows a causal link between high-level automation and automation complacency, as it describes how overreliance on automation decreases vigilance.",y
TowardsContext-AwareModelingofSituationAwarenessinConditionallyAutomatedDriving,Automation errors,Enhanced SA,"Automation errors lead to enhanced SA because they increase drivers' distrust and alertness, prompting them to maintain higher awareness.","""On the other hand, automation errors (i.e., false alarms and missed TORs) further elevated SA by increasing drivers’ distrust and alertness."" (p. 23)","This excerpt shows a causal link between automation errors and enhanced SA, as it describes how errors increase distrust and alertness, leading to higher awareness.",y
TowardsContext-AwareModelingofSituationAwarenessinConditionallyAutomatedDriving,High-level automation,Delayed takeover transitions,"High-level automation leads to delayed takeover transitions because drivers have difficulty resuming control, leading to confusion and errors in complex situations.","""Delayed takeover transitions: Smooth and timely transfer of control between the driver and the automated system is crucial for maintaining SA and avoiding accidents. Difficulty in resuming control can lead to confusion and errors, especially in complex or unexpected situations (Zhou et al., 2022)."" (p. 5)","This excerpt shows a causal link between high-level automation and delayed takeover transitions, as it describes how difficulty in resuming control leads to confusion and errors.",y
TowardsContext-AwareModelingofSituationAwarenessinConditionallyAutomatedDriving,Automation errors,Decreased trust,"Automation errors lead to decreased trust because they cause drivers to doubt the system's reliability, affecting their vigilance and SA.","""Previous research suggested both false alarms and misses can decrease user trust (Ayoub, Avetisian, Yang, & Zhou, 2023)."" (p. 12)","This excerpt shows a causal link between automation errors and decreased trust, as it describes how errors lead to doubts about system reliability.",y
amiajnl-2011-000089,Automation bias,Errors of commission and omission,"Automation bias leads to errors of commission and omission because users over-rely on incorrect DSS advice, reversing correct decisions or failing to act without prompts.","""Automation bias (AB)...manifests in errors of commission (following incorrect advice) and omission (failing to act because of not being prompted to do so) when using CDSS."" (p. 1)","This excerpt shows a direct link between automation bias and specific types of errors, highlighting how reliance on DSS can lead to incorrect actions or inaction.",y
amiajnl-2011-000089,High-level automation,Complacency,"High-level automation leads to complacency because users reduce monitoring of automation output, especially when the system is deemed reliable.","""Parasuraman et al note that commission and omission errors result from both AB and complacency...Complacency appears to occur as an attention allocation strategy in multitasking where manual tasks are attended to over monitoring the veracity of the output of automation."" (p. 1)","This excerpt explains how high-level automation can cause users to become complacent, neglecting the need to verify automation outputs.",y
amiajnl-2011-000089,Trust in DSS,Over-reliance on automation,"Trust in DSS increases over-reliance because users are more likely to accept DSS advice without verification, even when it is incorrect.","""Trust is possibly the strongest driving factor in over-reliance, when trust is incorrectly calibrated against system reliability."" (p. 4)","This excerpt identifies trust as a key factor in over-reliance, leading to acceptance of incorrect DSS outputs.",y
amiajnl-2011-000089,Position of advice on screen,Automation bias,Prominent display of advice increases automation bias because users are more likely to follow advice that is visually emphasized.,"""Berner et al found that display prominence increased AB, affecting the likelihood of a decision being changed after advice—prominent incorrect advice is more likely to be followed."" (p. 5)",This excerpt links the design feature of advice positioning to increased likelihood of automation bias.,y
amiajnl-2011-000089,Training,Reduction in automation bias,Training reduces automation bias by increasing user awareness of potential errors and encouraging verification of DSS outputs.,"""Papers have assessed the effect of training on appropriate DSS use; linked to experience discussed above, training may increase the likelihood of recognizing DSS error and thus reduce AB."" (p. 5)",This excerpt suggests that training can mitigate automation bias by enhancing user vigilance and error recognition.,y
fhumd-7-1579166,Advanced AI cognition in Centaurian systems,Blurring of boundaries between human and AI decision-making,"Advanced AI cognition in Centaurian systems leads to a blurring of boundaries between human and AI decision-making, which can cause issues of accountability and transparency.","""As hybrid systems increasingly share decision-making authority between human and AI components, especially in domains such as healthcare, defense, or finance, it becomes essential to ensure traceable paths of action and define clear loci of control."" (p. 2)","This excerpt shows that the integration of AI into decision-making processes can obscure accountability, which is a form of performance degradation in terms of oversight and responsibility.",y
fhumd-7-1579166,Agentic AI systems with iterative planning and autonomous task decomposition,Over-reliance on AI for decision-making,"Agentic AI systems with iterative planning and autonomous task decomposition can lead to over-reliance on AI for decision-making, reducing human engagement and critical thinking.","""Agentic AI systems engage in sophisticated reasoning and iterative planning to solve complex, multi-step problems autonomously."" (p. 4)","The excerpt suggests that AI systems are taking over complex problem-solving tasks, which may lead to humans becoming less involved or skilled in these areas.",y
fhumd-7-1579166,Integration of LLMs and human operators in Centaurian systems,Dependency on AI recommendations,"Integration of LLMs and human operators in Centaurian systems can lead to dependency on AI recommendations, potentially diminishing human decision-making skills.","""The interaction between human operators and the LLM exhibits Centaurian characteristics, creating a tightly coupled decision unit where human judgment and AI recommendations are seamlessly integrated."" (p. 10)","This indicates that the integration might lead to humans relying more on AI recommendations, which could degrade their independent decision-making abilities.",y
fhumd-7-1579166,Neuro-symbolic integration in LAMs,Complexity in human-AI interaction,"Neuro-symbolic integration in LAMs can increase complexity in human-AI interaction, potentially leading to confusion or errors in understanding AI outputs.","""By combining neural network capabilities for pattern recognition with symbolic AI for rule-based processing, LAM demonstrates how apparently distinct computational approaches can be unified into a coherent whole."" (p. 11)","The integration of different AI approaches can make the system more complex, which might confuse users and degrade performance.",y
frvir-02-686783,Conversational ability in robots,Increased mental demand and decreased performance,Conversational robots increase mental demand and decrease performance because they require additional cognitive resources for processing social interactions.,"""the pragmatic quality and the mental effort indicated a more negative evaluation of the conversational robot than the non-conversational robot."" (p. 10)","The excerpt shows that the conversational robot increased mental demand, which implies a cognitive overload leading to decreased performance.",y
frvir-02-686783,Complex appearance of AI systems,Perceived trustworthiness and friendliness differences,"Complex appearance affects perceived trustworthiness and friendliness because users attribute more intelligence to complex-looking systems, impacting their trust and interaction quality.","""The complex appearance led up to less perceived risk (a sign of higher trustworthiness). However, the simple appearance was rated friendlier, and more benevolent (in the correct condition)."" (p. 13)","The excerpt indicates that the appearance of AI systems influences user perceptions, which can lead to biases in trust and friendliness, affecting interaction outcomes.",y
frvir-02-686783,Embodiment of AI systems,Different user evaluations based on gender,Embodiment of AI systems leads to varied user evaluations based on gender because different genders perceive and interact with embodied AI differently.,"""Women rated the conversational robot as more positive (significant) and attractive (by trend) than the non-conversational robot, while men did not show any differences."" (p. 10)","The excerpt shows that the embodiment of AI systems can lead to gender-based differences in evaluation, which can impact the effectiveness of human-AI interaction.",y
